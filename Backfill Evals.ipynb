{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "43011e84-369c-4842-9306-df01f5682498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "118bbfd6-bc05-498d-8b1c-c9be31f0b368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "pandarallel.initialize(\n",
    "    nb_workers=min(multiprocessing.cpu_count()-1, 8),\n",
    "    progress_bar=True, verbose=1\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "K = 5  # Top-K merchants/categories to evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f551612-138a-4c51-9afe-c846bbde5ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 1. Confidence Score\n",
    "The confidence score, as calculated by the `top_k_rfm` function, measures how \"**confident**\" the model is in its top-K recommendations for a specific user. \n",
    "\n",
    "> It quantifies how much the top items stand out from the rest of the items for that user.\n",
    "\n",
    "\n",
    "\n",
    "`Confidence = (Sum of scores of Top-K items) / (Sum of scores of all items)`\n",
    "\n",
    "## What it Tells Us\n",
    " - A high confidence score (close to 1.0) means that the user's top-K items have significantly higher scores than their other items. This indicates a strong, concentrated preference, and we can be more confident that these top items are indeed the most important ones for this user.\n",
    "\n",
    "- A low confidence score (closer to 0.0) suggests that the scores are more evenly distributed across all items. The distinction between the top-K items and the rest is less clear, indicating a weaker or more diverse preference.\n",
    "\n",
    "\n",
    "# 2. Hit Rate\n",
    " The hit rate, calculated by the `compute_hit_scores` function, is a classic accuracy metric. \n",
    " \n",
    " > It evaluates how well the model's recommendations predict a user's future behavior. It answers the question: \"Of the K items we recommended, how many did the user actually interact with?\"\n",
    "\n",
    "How it's Calculated\n",
    "To calculate the hit rate, you first need to split your data (e.g., by time) into a training set and a test set.\n",
    "\n",
    "- Generate top-K recommendations using the training set.\n",
    "\n",
    "- Identify the actual top-K items the user interacted with in the test set.\n",
    " \n",
    "- The hit rate is the proportion of items that appear in both lists.\n",
    " \n",
    "- `Hit Rate = (Number of items in both Train Top-K and Test Top-K) / K\n",
    "`\n",
    "\n",
    "##### This is also a form of Recall @K.\n",
    "\n",
    "## What it Tells Us\n",
    "- The hit rate is a direct measure of the recommendation model's predictive power. A higher hit rate means the model is more effective at identifying items that the user will find relevant in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7d5d4b-4f25-42d9-87da-ca7e322187c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def top_k_rfm(df, user_col, item_col, score_col, K=5):\n",
    "    \"\"\"Return top-K items ranked by RFM score with confidence score.\"\"\"\n",
    "    def _get_top(x):\n",
    "        x_sorted = x.sort_values(score_col, ascending=False)\n",
    "        top_items = x_sorted.head(K)[item_col].tolist()\n",
    "        conf = x_sorted.head(K)[score_col].sum() / max(1, x_sorted[score_col].sum())\n",
    "        return pd.Series({\"top_items\": top_items, \"confidence\": conf})\n",
    "    return df.groupby(user_col, group_keys=False).progress_apply(_get_top).reset_index()\n",
    "\n",
    "def compute_hit_scores(train_top, test_top, item_label):\n",
    "    \"\"\"Compute hit score = overlap between train & test top-K sets (parallelized).\"\"\"\n",
    "    merged = pd.merge(train_top, test_top, on=\"user_id\", how=\"inner\", suffixes=(\"_train\", \"_test\"))\n",
    "\n",
    "    def _hit(row):\n",
    "        return len(set(row[\"top_items_train\"]).intersection(row[\"top_items_test\"])) / max(1, len(row[\"top_items_train\"]))\n",
    "\n",
    "    # Parallelized apply\n",
    "    merged[f\"{item_label}_hit_score\"] = merged.parallel_apply(_hit, axis=1)\n",
    "    merged = merged.rename(columns={\"confidence_train\": f\"{item_label}_confidence\"})\n",
    "    return merged[[\"user_id\", f\"{item_label}_confidence\", f\"{item_label}_hit_score\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef42b72a-74f1-4f6b-98da-fa9cab5226bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_spark = spark.read.csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_train.csv\", header=True, inferSchema=True)\n",
    "# train_df = train_spark.toPandas()  # convert only if memory allows\n",
    "\n",
    "# test_spark = spark.read.csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_test.csv\", header=True, inferSchema=True)\n",
    "# test_df = test_spark.toPandas()  # convert only if memory allows\n",
    "\n",
    "train_df = pd.read_csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_train.csv\")\n",
    "test_df = pd.read_csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(\"Train sample:\")\n",
    "display(train_df.head(10))\n",
    "print(\"Test sample:\")\n",
    "display(test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430d9fa4-947c-4e6c-8a44-c9bfdbc1282b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Computing merchant Top-K for train & test...\")\n",
    "train_merchants = top_k_rfm(train_df, \"user_id\", \"merchant_standardized\", \"RFM_score_merchant\", K)\n",
    "test_merchants  = top_k_rfm(test_df,  \"user_id\", \"merchant_standardized\", \"RFM_score_merchant\", K)\n",
    "\n",
    "print(\"Evaluating merchant hit scores...\")\n",
    "merchant_eval = compute_hit_scores(train_merchants, test_merchants, \"merchant\")\n",
    "\n",
    "print(\"‚úÖ Merchant evaluation complete\")\n",
    "display(merchant_eval.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fc8242-d1ca-4c7f-9f8d-8c50069c1354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Computing category Top-K for train & test...\")\n",
    "train_categories = top_k_rfm(train_df, \"user_id\", \"appcategory\", \"RFM_score_category\", K)\n",
    "test_categories  = top_k_rfm(test_df,  \"user_id\", \"appcategory\", \"RFM_score_category\", K)\n",
    "\n",
    "print(\"Evaluating category hit scores...\")\n",
    "category_eval = compute_hit_scores(train_categories, test_categories, \"category\")\n",
    "\n",
    "print(\"‚úÖ Category evaluation complete\")\n",
    "display(category_eval.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0408ed30-fc76-4b3c-9d45-2c09aa461f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_report = (\n",
    "    merchant_eval\n",
    "    .merge(category_eval, on=\"user_id\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# Add segments from training\n",
    "seg_merchants = train_df.groupby(\"user_id\")[\"customer_segment_merchant\"].first().reset_index()\n",
    "seg_categories = train_df.groupby(\"user_id\")[\"customer_segment_category\"].first().reset_index()\n",
    "\n",
    "user_report = (\n",
    "    user_report\n",
    "    .merge(seg_merchants.rename(columns={\"customer_segment_merchant\": \"merchant_segment\"}), on=\"user_id\", how=\"left\")\n",
    "    .merge(seg_categories.rename(columns={\"customer_segment_category\": \"category_segment\"}), on=\"user_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"User report sample:\")\n",
    "display(user_report.head(10))\n",
    "\n",
    "# üíæ Save User Report\n",
    "output_path = \"/Volumes/jupiter/temp/temp/rfm_backfill_user_report.csv\"\n",
    "user_report.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Backfill evaluation report saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b491a9-7ed5-4ccc-a8e1-88d40b399637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hit rate summary\n",
    "hit_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"Merchant\", \"Category\"],\n",
    "    \"Hit Rate\": [\n",
    "        merchant_eval[\"merchant_hit_score\"].mean(),\n",
    "        category_eval[\"category_hit_score\"].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.barplot(x=\"Metric\", y=\"Hit Rate\", data=hit_summary, palette=\"viridis\")\n",
    "plt.title(f\"Top-{K} Hit Rate Comparison\", fontsize=14, weight=\"bold\")\n",
    "plt.ylim(0,1)\n",
    "for i, v in enumerate(hit_summary[\"Hit Rate\"]):\n",
    "    plt.text(i, v+0.02, f\"{v:.2%}\", ha=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# Confidence distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(merchant_eval[\"merchant_confidence\"], bins=30, color=\"blue\", label=\"Merchant\", kde=True)\n",
    "sns.histplot(category_eval[\"category_confidence\"], bins=30, color=\"green\", label=\"Category\", kde=True)\n",
    "plt.title(\"Confidence Score Distribution\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Confidence Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Segment-level hit rate\n",
    "merchant_seg_eval = user_report.groupby(\"merchant_segment\")[\"merchant_hit_score\"].mean().reset_index()\n",
    "category_seg_eval = user_report.groupby(\"category_segment\")[\"category_hit_score\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=merchant_seg_eval.sort_values(\"merchant_hit_score\", ascending=False),\n",
    "            x=\"merchant_hit_score\", y=\"merchant_segment\", palette=\"Blues_r\")\n",
    "plt.title(\"Merchant Hit Rate by Segment\", fontsize=14, weight=\"bold\")\n",
    "plt.xlim(0,1)\n",
    "for i, v in enumerate(merchant_seg_eval.sort_values(\"merchant_hit_score\", ascending=False)[\"merchant_hit_score\"]):\n",
    "    plt.text(v+0.01, i, f\"{v:.2%}\", va=\"center\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=category_seg_eval.sort_values(\"category_hit_score\", ascending=False),\n",
    "            x=\"category_hit_score\", y=\"category_segment\", palette=\"Greens_r\")\n",
    "plt.title(\"Category Hit Rate by Segment\", fontsize=14, weight=\"bold\")\n",
    "plt.xlim(0,1)\n",
    "for i, v in enumerate(category_seg_eval.sort_values(\"category_hit_score\", ascending=False)[\"category_hit_score\"]):\n",
    "    plt.text(v+0.01, i, f\"{v:.2%}\", va=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9c282a-8237-4fee-ac08-cb0ccaa6d2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# üìä Confidence vs Hit Score Scatter (Quadrant Analysis)\n",
    "# ================================\n",
    "\n",
    "# Prepare data (combine merchant + category for plotting)\n",
    "scatter_df = (\n",
    "    user_report[[\"user_id\", \"merchant_confidence\", \"merchant_hit_score\", \n",
    "                 \"category_confidence\", \"category_hit_score\"]]\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# Plot for merchants\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=scatter_df.sample(min(5000, len(scatter_df))),  # sample for readability if too big\n",
    "    x=\"merchant_confidence\", y=\"merchant_hit_score\",\n",
    "    alpha=0.4, edgecolor=None\n",
    ")\n",
    "plt.axhline(0.5, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.axvline(0.5, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Confidence vs Hit Score (Merchant-Level)\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Confidence (Train RFM Concentration)\")\n",
    "plt.ylabel(\"Hit Score (Train vs Test Overlap)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for categories\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=scatter_df.sample(min(5000, len(scatter_df))),\n",
    "    x=\"category_confidence\", y=\"category_hit_score\",\n",
    "    alpha=0.4, edgecolor=None, color=\"green\"\n",
    ")\n",
    "plt.axhline(0.5, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.axvline(0.5, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Confidence vs Hit Score (Category-Level)\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Confidence (Train RFM Concentration)\")\n",
    "plt.ylabel(\"Hit Score (Train vs Test Overlap)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14128ce3-a8a6-49fe-abae-db98f870e53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Numerical Summaries\")\n",
    "\n",
    "# Average hit rates\n",
    "avg_hit_merchant = merchant_eval[\"merchant_hit_score\"].mean()\n",
    "avg_hit_category = category_eval[\"category_hit_score\"].mean()\n",
    "\n",
    "# Average confidence\n",
    "avg_conf_merchant = merchant_eval[\"merchant_confidence\"].mean()\n",
    "avg_conf_category = category_eval[\"category_confidence\"].mean()\n",
    "\n",
    "print(f\"\\nMerchant Hit Rate (avg): {avg_hit_merchant:.2%}\")\n",
    "print(f\"Category Hit Rate (avg): {avg_hit_category:.2%}\")\n",
    "print(f\"\\nMerchant Confidence (avg): {avg_conf_merchant:.2f}\")\n",
    "print(f\"Category Confidence (avg): {avg_conf_category:.2f}\")\n",
    "\n",
    "# Quadrant counts (merchant-level)\n",
    "def quadrant(row):\n",
    "    if row[\"merchant_confidence\"] >= 0.5 and row[\"merchant_hit_score\"] >= 0.5:\n",
    "        return \"Good Predictions\"\n",
    "    elif row[\"merchant_confidence\"] >= 0.5 and row[\"merchant_hit_score\"] < 0.5:\n",
    "        return \"False Confident\"\n",
    "    elif row[\"merchant_confidence\"] < 0.5 and row[\"merchant_hit_score\"] >= 0.5:\n",
    "        return \"Noisy / Lucky\"\n",
    "    else:\n",
    "        return \"Unpredictable\"\n",
    "\n",
    "user_report[\"merchant_quadrant\"] = user_report.apply(quadrant, axis=1)\n",
    "\n",
    "quad_counts = user_report[\"merchant_quadrant\"].value_counts(normalize=False).reset_index()\n",
    "quad_counts.columns = [\"Quadrant\", \"Users\"]\n",
    "\n",
    "quad_counts[\"Percentage\"] = quad_counts[\"Users\"] / len(user_report) * 100\n",
    "print(\"\\nMerchant Quadrant Distribution:\")\n",
    "display(quad_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb305b19-3f57-4419-9c90-3c43d5bf929b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# üè∑Ô∏è Segment-Level Analysis\n",
    "# ================================\n",
    "\n",
    "# Merchant segments\n",
    "merchant_seg_eval = user_report.groupby(\"merchant_segment\")[[\"merchant_hit_score\",\"merchant_confidence\"]].mean().reset_index()\n",
    "\n",
    "# Category segments\n",
    "category_seg_eval = user_report.groupby(\"category_segment\")[[\"category_hit_score\",\"category_confidence\"]].mean().reset_index()\n",
    "\n",
    "print(\"Merchant Segment Averages:\")\n",
    "display(merchant_seg_eval)\n",
    "\n",
    "print(\"Category Segment Averages:\")\n",
    "display(category_seg_eval)\n",
    "\n",
    "# Plot merchant segment hit rate\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=merchant_seg_eval.sort_values(\"merchant_hit_score\", ascending=False),\n",
    "            x=\"merchant_hit_score\", y=\"merchant_segment\", palette=\"Blues_r\")\n",
    "plt.title(\"Merchant Hit Rate by Segment\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Avg Hit Rate\")\n",
    "plt.ylabel(\"Merchant Segment\")\n",
    "plt.xlim(0,1)\n",
    "for i,v in enumerate(merchant_seg_eval.sort_values(\"merchant_hit_score\", ascending=False)[\"merchant_hit_score\"]):\n",
    "    plt.text(v+0.01, i, f\"{v:.2%}\", va=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# Plot category segment hit rate\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=category_seg_eval.sort_values(\"category_hit_score\", ascending=False),\n",
    "            x=\"category_hit_score\", y=\"category_segment\", palette=\"Greens_r\")\n",
    "plt.title(\"Category Hit Rate by Segment\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Avg Hit Rate\")\n",
    "plt.ylabel(\"Category Segment\")\n",
    "plt.xlim(0,1)\n",
    "for i,v in enumerate(category_seg_eval.sort_values(\"category_hit_score\", ascending=False)[\"category_hit_score\"]):\n",
    "    plt.text(v+0.01, i, f\"{v:.2%}\", va=\"center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a06b568-5c52-4fd6-8099-593d8737f30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# üîé Per-User Drilldown Function\n",
    "# ================================\n",
    "def user_drilldown(user_id):\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üîé User: {user_id}\")\n",
    "    \n",
    "    # Merchant metrics\n",
    "    row_m = merchant_eval[merchant_eval[\"user_id\"] == user_id]\n",
    "    row_c = category_eval[category_eval[\"user_id\"] == user_id]\n",
    "    \n",
    "    if not row_m.empty:\n",
    "        print(f\"\\nMerchant Confidence: {row_m['merchant_confidence'].values[0]:.2f}\")\n",
    "        print(f\"Merchant Hit Score: {row_m['merchant_hit_score'].values[0]:.2f}\")\n",
    "    if not row_c.empty:\n",
    "        print(f\"\\nCategory Confidence: {row_c['category_confidence'].values[0]:.2f}\")\n",
    "        print(f\"Category Hit Score: {row_c['category_hit_score'].values[0]:.2f}\")\n",
    "\n",
    "    # Show Top-K from train & test (merchants)\n",
    "    train_top_m = train_merchants[train_merchants[\"user_id\"] == user_id]\n",
    "    test_top_m  = test_merchants[test_merchants[\"user_id\"] == user_id]\n",
    "    if not train_top_m.empty:\n",
    "        print(\"\\nTrain Top Merchants:\", train_top_m[\"top_items\"].values[0])\n",
    "    if not test_top_m.empty:\n",
    "        print(\"Test Top Merchants:\", test_top_m[\"top_items\"].values[0])\n",
    "\n",
    "    # Show Top-K from train & test (categories)\n",
    "    train_top_c = train_categories[train_categories[\"user_id\"] == user_id]\n",
    "    test_top_c  = test_categories[test_categories[\"user_id\"] == user_id]\n",
    "    if not train_top_c.empty:\n",
    "        print(\"\\nTrain Top Categories:\", train_top_c[\"top_items\"].values[0])\n",
    "    if not test_top_c.empty:\n",
    "        print(\"Test Top Categories:\", test_top_c[\"top_items\"].values[0])\n",
    "\n",
    "# Example usage\n",
    "# user_drilldown(\"0000b6d5-f969-4996-ac9c-0635f1eed680\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64315b1-8daa-4271-90ab-69e346126cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Backfill Evals",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
