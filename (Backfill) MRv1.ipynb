{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472bafc8-58dc-473d-8fc2-31515cae94c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Merchant Recommendation System (MRv1)\n",
    "\n",
    "### Notebook Flow\n",
    "\n",
    "1. Data Loading (Spark -> CSV source of truth)\n",
    "2. EDA\n",
    "3. Listing Merchants\n",
    "4. Cleaning and Extraction\n",
    "5. Standardization\n",
    "6. Grouping\n",
    "7. Categorization (MCC + Pattern + Fallback)\n",
    "8. RFM Analysis\n",
    "9. Category-Level RFM\n",
    "10. Final Integration & Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea944953-c145-4e0c-a5bf-8f64cc5ba52a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import time\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from pandarallel import pandarallel\n",
    "    PANDARALLEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PANDARALLEL_AVAILABLE = False\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if PANDARALLEL_AVAILABLE:\n",
    "    pandarallel.initialize(nb_workers=min(multiprocessing.cpu_count()-1, 8), progress_bar=False, verbose=0)\n",
    "\n",
    "# Use this canonical path for all file operations (pandas, CSV, etc.)\n",
    "VOLUME_PATH = \"/Volumes/jupiter/temp/temp/\"\n",
    "Path(VOLUME_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_TABLE = \"jupiter.temp.backfill_test_mrinal_v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d734933-01bf-4c40-8ac3-f432a9f41a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Loading (Spark -> CSV source of truth)\n",
    "\n",
    "We first load from the Databricks table and immediately persist to CSV under a shared volume. Subsequent steps read/write CSVs and also update the in-memory DataFrame so outputs remain visible in notebook runs.\n",
    "\n",
    "1. `jupiter.temp.unified_transaction_table_v2` : Base table consisting of all {User - Merchant} pairs where the Amount(txn.) > 100\n",
    "2. `jupiter.temp.unique_user_merchant_mapping` : Subsequent created table for all {U-M} pairs where Count(Txns.) > 2 from the base table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacfac03-af5c-4dee-9b28-aa0de854fdbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load from Spark, persist to CSV, and use CSV as source of truth\n",
    "print(f\"üìã Loading from table: {DATA_TABLE}\")\n",
    "sample_df = (\n",
    "    spark.table(DATA_TABLE)\n",
    "         .toPandas()\n",
    ")\n",
    "\n",
    "csv_path = f\"{VOLUME_PATH}mrv1_backfill.csv\"\n",
    "sample_df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Saved raw snapshot to: {csv_path}\")\n",
    "\n",
    "# Re-load from CSV as source of truth\n",
    "sample_df = pd.read_csv(csv_path)\n",
    "print(f\"‚úÖ Reloaded from CSV: {sample_df.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(f\"üë• Users: {sample_df['user_id'].nunique() if 'user_id' in sample_df.columns else 'N/A'}\")\n",
    "print(f\"üè™ Merchants (raw): {sample_df['merchant'].nunique() if 'merchant' in sample_df.columns else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40449cb-a8b0-4857-a2fa-8964150d55c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EDA & Listing Merchants\n",
    "\n",
    "We inspect dataset shape, status, and list top merchants (raw vs extracted). All previews are saved to CSVs in the shared volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1184ab7b-46ab-468f-877e-a55ecb16fcfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA quick\n",
    "print(f\"Shape: {sample_df.shape}\")\n",
    "if 'transactionstatus' in sample_df.columns:\n",
    "    print(sample_df['transactionstatus'].value_counts().to_string())\n",
    "\n",
    "# Top merchants (raw)\n",
    "if 'merchant' in sample_df.columns:\n",
    "    top_raw = sample_df['merchant'].fillna('Unknown').astype(str).str.strip().str.lower().value_counts().head(50)\n",
    "    raw_path = f\"{VOLUME_PATH}mrv1_top_merchants_backfill.csv\"\n",
    "    top_raw.to_csv(raw_path)\n",
    "    print(f\"‚úÖ Saved top raw merchants to {raw_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da6ad18-c002-4a11-b97c-5023d36f07a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Cleaning and Extraction\n",
    "\n",
    "We clean rows, filter successful txns, and extract merchant names from embedded JSON structures, then persist the cleaned snapshot to CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf27b35-90a4-4505-b288-7f0607dc12c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## User-Merchant Transaction Analysis\n",
    "\n",
    "**Dataset**: `jupiter.temp.unified_transactions_v2` - Transaction level data for all users including Fact MM + Stg Rewards + ULM CC table\n",
    "\n",
    "**Technical Pipeline**:\n",
    "1. Data loading and exploratory analysis\n",
    "2. Merchant name standardization\n",
    "3. User-merchant pair aggregation\n",
    "4. Merchant categorization (hybrid approach)\n",
    "5. Transaction pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf16c54-eb26-4658-a7b8-c58196371a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Sample Information:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Columns: {len(sample_df.columns)}\")\n",
    "print(f\"Sample rows: {len(sample_df)}\")\n",
    "\n",
    "# Display column information\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(\"-\" * 50)\n",
    "for col, dtype in sample_df.dtypes.items():\n",
    "    non_null = sample_df[col].count()\n",
    "    pct_filled = non_null / len(sample_df) * 100\n",
    "    print(f\"{col:<25} {str(dtype):<12} {non_null:>8,} non-null ({pct_filled:.1f}%)\")\n",
    "\n",
    "# Merchant columns for analysis\n",
    "merchant_cols = [col for col in sample_df.columns if any(k in col.lower() for k in ['merchant', 'payee'])]\n",
    "print(f\"\\nMerchant columns: {merchant_cols}\")\n",
    "\n",
    "# Preview data\n",
    "sample_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34c03fd-29bd-41b8-876e-c307804e4937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Analyze key metrics\n",
    "print(\"\\nKey Dataset Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Unique users: {sample_df['user_id'].nunique():,}\")\n",
    "print(f\"Unique merchants: {sample_df['merchant'].nunique():,}\")\n",
    "\n",
    "# Transaction amount stats\n",
    "print(f\"\\nTransaction amount stats:\")\n",
    "print(f\"  - Min: ‚Çπ{sample_df['total_spend'].min():,.2f}\")\n",
    "print(f\"  - Max: ‚Çπ{sample_df['total_spend'].max():,.2f}\")\n",
    "print(f\"  - Mean: ‚Çπ{sample_df['total_spend'].mean():,.2f}\")\n",
    "print(f\"  - Median: ‚Çπ{sample_df['total_spend'].median():,.2f}\")\n",
    "print(f\"  - Total: ‚Çπ{sample_df['total_spend'].sum():,.2f}\")\n",
    "\n",
    "# Transaction count stats\n",
    "print(f\"\\nTransaction count stats:\")\n",
    "print(f\"  - Min: {sample_df['total_txns'].min():,}\")\n",
    "print(f\"  - Max: {sample_df['total_txns'].max():,}\")\n",
    "print(f\"  - Mean: {sample_df['total_txns'].mean():.2f}\")\n",
    "print(f\"  - Median: {sample_df['total_txns'].median():,}\")\n",
    "print(f\"  - Total: {sample_df['total_txns'].sum():,}\")\n",
    "\n",
    "# Analyze transaction sources\n",
    "print(\"\\nTransaction Sources:\")\n",
    "print(\"-\" * 50)\n",
    "source_counts = sample_df['source'].value_counts()\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"{source:<10}: {count:,} records ({count/len(sample_df):.1%})\")\n",
    "\n",
    "# Analyze transaction status\n",
    "print(\"\\nTransaction Status:\")\n",
    "print(\"-\" * 50)\n",
    "status_counts = sample_df['transactionstatus'].value_counts()\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"{status:<10}: {count:,} records ({count/len(sample_df):.1%})\")\n",
    "\n",
    "# Check date range\n",
    "print(\"\\nTransaction Date Range:\")\n",
    "print(\"-\" * 50)\n",
    "sample_df['first_txn_date'] = pd.to_datetime(sample_df['first_txn_date'])\n",
    "sample_df['last_txn_date'] = pd.to_datetime(sample_df['last_txn_date'])\n",
    "min_date = sample_df['first_txn_date'].min()\n",
    "max_date = sample_df['last_txn_date'].max()\n",
    "date_range = max_date - min_date\n",
    "print(f\"Earliest: {min_date}\")\n",
    "print(f\"Latest: {max_date}\")\n",
    "print(f\"Range: {date_range.days} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5918169f-16b0-4324-99f6-a969ccafa07f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Visualize transaction distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Transactions per user distribution (log scale)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(sample_df.groupby('user_id')['total_txns'].sum(), log_scale=True)\n",
    "plt.title('Transactions per User (Log Scale)')\n",
    "plt.xlabel('Number of Transactions')\n",
    "plt.ylabel('Count of Users')\n",
    "\n",
    "# Plot 2: Spend per user distribution (log scale)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(sample_df.groupby('user_id')['total_spend'].sum(), log_scale=True)\n",
    "plt.title('Total Spend per User (Log Scale)')\n",
    "plt.xlabel('Total Spend (‚Çπ)')\n",
    "plt.ylabel('Count of Users')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display dataset shape\n",
    "print(f\"Dataset shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd96499-9ba5-4657-bdd3-5abac1a0a689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Prep \n",
    "\n",
    "**Key Steps:**\n",
    "1. **Data Cleaning**: Filter invalid records and handle missing values\n",
    "2. **Merchant Name Extraction**: Extract merchant names from JSON formats\n",
    "3. **Merchant Name Standardization**: Remove prefixes/suffixes, normalize formatting\n",
    "4. **Source Analysis**: Analyze MM vs CC transaction distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978d9e24-8709-4162-a2bb-897052c4fbd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(\"Cleaning and preparing data...\")\n",
    "sample_df_clean = sample_df.dropna(subset=['merchant']).copy() # Filter out records with missing merchant names\n",
    "print(f\"Removed {len(sample_df) - len(sample_df_clean):,} records with missing merchant names\")\n",
    "\n",
    "# Filter out failed transactions\n",
    "if 'transactionstatus' in sample_df_clean.columns:\n",
    "    success_mask = sample_df_clean['transactionstatus'] == 'SUCCESS'\n",
    "    sample_df_clean = sample_df_clean[success_mask].copy()\n",
    "    print(f\"Kept only successful transactions: {len(sample_df_clean):,} records\")\n",
    "\n",
    "# Extract merchant name from JSON format if present\n",
    "def extract_merchant_name(merchant_str):\n",
    "    if pd.isna(merchant_str) or not isinstance(merchant_str, str):\n",
    "        return merchant_str\n",
    "    \n",
    "    # Extract name from JSON format if present\n",
    "    json_pattern = r'\"name\":\"([^\"]+)\"'\n",
    "    json_match = re.search(json_pattern, merchant_str)\n",
    "    if json_match:\n",
    "        return json_match.group(1).strip()\n",
    "    \n",
    "    return merchant_str\n",
    "\n",
    "# Apply extraction to merchant column\n",
    "sample_df_clean['merchant_extracted'] = sample_df_clean['merchant'].apply(extract_merchant_name)\n",
    "\n",
    "# Analyze source distribution\n",
    "source_counts = sample_df_clean['source'].value_counts()\n",
    "total_records = len(sample_df_clean)\n",
    "\n",
    "print(\"\\nTransaction Source Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"{source:<10}: {count:,} records ({count/total_records:.1%})\")\n",
    "\n",
    "# Check MCC code availability\n",
    "mcc_available = sample_df_clean['mcccode'].notna().sum()\n",
    "print(f\"\\nMCC code availability: {mcc_available:,} records ({mcc_available/total_records:.1%})\")\n",
    "\n",
    "# Show sample of extracted merchant names\n",
    "print(\"\\nSample of extracted merchant names:\")\n",
    "display(sample_df_clean[['merchant', 'merchant_extracted']].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11848eff-5092-46fe-b4a4-072b01964406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merchant Analysis\n",
    "\n",
    "**Key Metrics:**\n",
    "1. **User Reach**: Number of unique users per merchant\n",
    "2. **Transaction Frequency**: Total transaction count per merchant\n",
    "3. **Revenue Impact**: Total spend amount per merchant\n",
    "4. **Transaction Patterns**: Average spend, transactions per user\n",
    "5. **Merchant Popularity**: Ranking based on combined metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86fab00-9426-4395-bb74-5a520396adf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Analyze top merchants by different metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9a40e8-4c96-4048-8c01-2a999d38c9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by merchant and calculate key metrics\n",
    "merchant_metrics = sample_df_clean.groupby('merchant_extracted').agg({\n",
    "    'user_id': 'nunique',      # Distinct users\n",
    "    'total_txns': 'sum',      # Total transactions\n",
    "    'total_spend': 'sum',     # Total spend\n",
    "    'mcccode': lambda x: x.mode().iloc[0] if not x.isna().all() else np.nan,  # Most common MCC code\n",
    "    'source': lambda x: x.mode().iloc[0]  # Most common source\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "merchant_metrics.columns = ['merchant', 'users', 'transactions', 'spend', 'mcc_code', 'primary_source']\n",
    "\n",
    "# Calculate derived metrics\n",
    "merchant_metrics['avg_txn_amount'] = (merchant_metrics['spend'] / merchant_metrics['transactions']).round(2)\n",
    "merchant_metrics['txn_per_user'] = (merchant_metrics['transactions'] / merchant_metrics['users']).round(2)\n",
    "merchant_metrics['spend_per_user'] = (merchant_metrics['spend'] / merchant_metrics['users']).round(2)\n",
    "\n",
    "# Calculate popularity score (weighted combination of users, transactions, and spend)\n",
    "merchant_metrics['user_rank'] = merchant_metrics['users'].rank(ascending=False)\n",
    "merchant_metrics['txn_rank'] = merchant_metrics['transactions'].rank(ascending=False)\n",
    "merchant_metrics['spend_rank'] = merchant_metrics['spend'].rank(ascending=False)\n",
    "merchant_metrics['popularity_score'] = (0.4 * merchant_metrics['user_rank'] + \n",
    "                                      0.3 * merchant_metrics['txn_rank'] + \n",
    "                                      0.3 * merchant_metrics['spend_rank'])\n",
    "\n",
    "# Display top merchants by different metrics\n",
    "print(\"Top 20 Merchants by User Reach:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<5}{'Merchant':<30}{'Users':<10}{'Transactions':<15}{'Spend (‚Çπ)':<15}{'Avg Txn (‚Çπ)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_by_users = merchant_metrics.sort_values('users', ascending=False).head(20)\n",
    "for i, (_, row) in enumerate(top_by_users.iterrows(), 1): \n",
    "    print(f\"{i:<5}{row['merchant'][:29]:<30}{row['users']:<10,}{row['transactions']:<15,}{row['spend']:<15,.2f}{row['avg_txn_amount']:<15,.2f}\")\n",
    "\n",
    "print(\"\\nTop 20 Merchants by Transaction Volume:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<5}{'Merchant':<30}{'Transactions':<15}{'Users':<10}{'Spend (‚Çπ)':<15}{'Txn/User':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_by_txns = merchant_metrics.sort_values('transactions', ascending=False).head(20)\n",
    "for i, (_, row) in enumerate(top_by_txns.iterrows(), 1):\n",
    "    print(f\"{i:<5}{row['merchant'][:29]:<30}{row['transactions']:<15,}{row['users']:<10,}{row['spend']:<15,.2f}{row['txn_per_user']:<10.2f}\")\n",
    "\n",
    "print(\"\\nTop 20 Merchants by Total Spend:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<5}{'Merchant':<30}{'Spend (‚Çπ)':<15}{'Users':<10}{'Transactions':<15}{'Spend/User (‚Çπ)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "top_by_spend = merchant_metrics.sort_values('spend', ascending=False).head(20)\n",
    "for i, (_, row) in enumerate(top_by_spend.iterrows(), 1):\n",
    "    print(f\"{i:<5}{row['merchant'][:29]:<30}{row['spend']:<15,.2f}{row['users']:<10,}{row['transactions']:<15,}{row['spend_per_user']:<15,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d014f4e-4647-4581-a63a-a90e90642ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Top 50 Merchants by User Reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08682eb-b43c-40ea-8e42-4d73ae7d1e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_by_users = (\n",
    "    merchant_metrics.sort_values('users', ascending=False)\n",
    "    .head(50)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"{'Rank':<5}{'Merchant':<30}{'Users':<12}{'Transactions':<15}{'Spend (‚Çπ)':<15}{'Avg Txn (‚Çπ)':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, row in top_by_users.iterrows():\n",
    "    print(f\"{i+1:<5}{row['merchant'][:29]:<30}{row['users']:<12,}{row['transactions']:<15,}{row['spend']:<15,.2f}{row['avg_txn_amount']:<15,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97966cb8-9064-4732-ae73-d7f7032d6141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Top 50 Merchants by Transaction Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace8dd52-7225-45cc-9d84-ec6e476ddb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_by_txns = (\n",
    "    merchant_metrics.sort_values('transactions', ascending=False)\n",
    "    .head(50)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"{'Rank':<5}{'Merchant':<30}{'Transactions':<15}{'Users':<12}{'Spend (‚Çπ)':<15}{'Txn/User':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, row in top_by_txns.iterrows():\n",
    "    print(f\"{i+1:<5}{row['merchant'][:29]:<30}{row['transactions']:<15,}{row['users']:<12,}{row['spend']:<15,.2f}{row['txn_per_user']:<12.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74416d52-c077-4142-8452-9fff0c08579e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Top 50 Merchants by Total Spend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf65bf7-014d-4c98-92ba-a77c5e8dabad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_by_spend = (\n",
    "    merchant_metrics.sort_values('spend', ascending=False)\n",
    "    .head(50)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"{'Rank':<5}{'Merchant':<30}{'Spend (‚Çπ)':<15}{'Users':<12}{'Transactions':<15}{'Spend/User (‚Çπ)':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, row in top_by_spend.iterrows():\n",
    "    print(f\"{i+1:<5}{row['merchant'][:29]:<30}{row['spend']:<15,.2f}{row['users']:<12,}{row['transactions']:<15,}{row['spend_per_user']:<15,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9436fe2-d77b-4b12-83ee-f352b1f0c35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merchant Name Standardization and Grouping\n",
    "\n",
    "The merchant standardization process consolidates different variations of the same merchant into a single canonical name. This is crucial for accurate merchant analysis as it:\n",
    "\n",
    "1. **Reduces data fragmentation** - Combines multiple variants of the same merchant (e.g., \"Swiggy Ltd\", \"SWIGGY\", \"Swiggy Online Order\" ‚Üí \"Swiggy\")\n",
    "2. **Improves data quality** - Removes inconsistencies in merchant names due to different sources (MM vs CC)\n",
    "3. **Enables accurate aggregation** - Allows proper grouping of transactions by merchant for reliable metrics\n",
    "4. **Facilitates categorization** - Makes it easier to categorize merchants by business type\n",
    "\n",
    "The standardization uses multiple techniques:\n",
    "- JSON extraction for structured merchant data\n",
    "- Regex pattern matching for common prefixes/suffixes\n",
    "- Explicit mappings for well-known merchants\n",
    "- Special character and formatting normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4675f8d4-c3e0-411a-91a6-573038c24acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# First, let's examine merchant name variations for common merchants\n",
    "merchant_keywords = ['swiggy', 'amazon', 'flipkart', 'zomato', 'phonepe']\n",
    "    \n",
    "for keyword in merchant_keywords:\n",
    "    # Use merchant_extracted instead of merchant_unified since we're working with the new dataset structure\n",
    "    variants = sample_df_clean[sample_df_clean['merchant_extracted'].str.contains(keyword, case=False, na=False)]\n",
    "    variant_counts = variants['merchant_extracted'].value_counts().head(5)\n",
    "    \n",
    "    print(f\"\\n{keyword.upper()} variants:\")\n",
    "    for name, count in variant_counts.items():\n",
    "        print(f\"- {name}: {count} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "eed140ef-6eaf-40cd-9c25-f319e634817c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b1c64b-a8da-400a-923b-daa51d8300aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Standardization function \n",
    "`standardize_merchant_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e36134a-5aba-4ca4-840d-ae273f5b7d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df0bac6-955b-4f18-a885-d9b3a9d5fec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initialize pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=4)\n",
    "\n",
    "def standardize_merchant_name(name):\n",
    "    \"\"\"\n",
    "    Standardize merchant names and filter out banks, competitors, and specified merchants.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Raw merchant name\n",
    "        \n",
    "    Returns:\n",
    "        str: Standardized merchant name or None if merchant should be filtered out\n",
    "    \"\"\"\n",
    "    if pd.isna(name) or not isinstance(name, str):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Extract name from JSON format if present\n",
    "    json_pattern = r'\"name\":\"([^\"]+)\"'\n",
    "    json_match = re.search(json_pattern, str(name))\n",
    "    if json_match:\n",
    "        name = json_match.group(1)\n",
    "    \n",
    "    # Basic cleaning\n",
    "    name = str(name).strip().lower()\n",
    "    name = re.sub(r'[^\\w\\s]', ' ', name)  # Replace special chars with space\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()  # Normalize whitespace\n",
    "    \n",
    "    # Merchants to filter out (existing + new list merged)\n",
    "    merchants_to_remove = {\n",
    "        # Existing\n",
    "        'vi', 'jio', 'airtel', 'irctc', 'indian oil', 'bsnl', 'bpcl', 'hpcl',\n",
    "        'phonepe', 'paytm', 'paytm wallet', 'paytm cash', 'paytm cashback', 'paytm m', 'paytm m-w',\n",
    "        'cred', 'groww', 'mobikwik', 'kiwi', 'supermoney', 'google pay',\n",
    "        'bharat connect utilities', 'mpokket', 'kreditbee', 'branch', 'on demand salary',\n",
    "        're cash', 'wdl', 'atw', 'mbk', 'ccbp', 'googlepay', 'bharatpe', 'capitalfloat',\n",
    "        'slice', 'navi', 'rblmycard'\n",
    "        \n",
    "        # Newly added merchants to filter\n",
    "        'debit card annual fee',\n",
    "        'google india digital',\n",
    "        'google i',\n",
    "        'vodafone idea',\n",
    "        'onecard',\n",
    "        'bank acc',\n",
    "        'sbi card',\n",
    "        'cheq',\n",
    "        'indian railways',\n",
    "        'bajaj finance',\n",
    "        'google india di',\n",
    "        'dummy name',\n",
    "        'zerodha',\n",
    "        'sbimops',\n",
    "        'indian railways catering and tourism',\n",
    "        'zerodha broking',\n",
    "        'cheq1 yesbank',\n",
    "        'atm cash',\n",
    "        'indmoney',\n",
    "        'snapmint',\n",
    "        'bank'\n",
    "    }\n",
    "    \n",
    "    if any(remove in name for remove in merchants_to_remove):\n",
    "        return None\n",
    "    \n",
    "    # Merchant name standardization mappings (unchanged)\n",
    "    merchant_mappings = {\n",
    "        # Food & Dining\n",
    "        'swiggy': 'Swiggy',\n",
    "        'instamart': 'Swiggy',\n",
    "        'zomato': 'Zomato',\n",
    "        'district': 'Zomato',\n",
    "        'dominos': 'Dominos Pizza',\n",
    "        'pizza hut': 'Pizza Hut',\n",
    "        'mcdonald': 'McDonalds',\n",
    "        'kfc': 'KFC',\n",
    "        'subway': 'Subway',\n",
    "        'burger king': 'Burger King',\n",
    "        'dunkin': 'Dunkin Donuts',\n",
    "        'starbucks': 'Starbucks',\n",
    "        'cafe coffee day': 'Cafe Coffee Day',\n",
    "        'ccd': 'Cafe Coffee Day',\n",
    "        \n",
    "        # E-commerce\n",
    "        'amazon': 'Amazon',\n",
    "        'flipkart': 'Flipkart',\n",
    "        'ekart': 'Flipkart',\n",
    "        'myntra': 'Myntra',\n",
    "        'ajio': 'AJIO',\n",
    "        'tatacliq': 'Tata CLiQ',\n",
    "        'meesho': 'Meesho',\n",
    "        'nykaa': 'Nykaa',\n",
    "        'firstcry': 'FirstCry',\n",
    "        \n",
    "        # Grocery & Supermarkets\n",
    "        'dmart': 'DMart',\n",
    "        'avenue supermarts': 'DMart',\n",
    "        'blinkit': 'Blinkit',\n",
    "        'grofers': 'Blinkit',\n",
    "        'bigbasket': 'BigBasket',\n",
    "        'bb now': 'BigBasket',\n",
    "        'zepto': 'Zepto',\n",
    "        'jiomart': 'JioMart',\n",
    "        'reliance smart': 'JioMart',\n",
    "        'reliance fresh': 'JioMart',\n",
    "        'more retail': 'More',\n",
    "        'spencers': 'Spencers',\n",
    "        'nature basket': 'Natures Basket',\n",
    "        \n",
    "        # Entertainment\n",
    "        'netflix': 'Netflix',\n",
    "        'hotstar': 'Disney+ Hotstar',\n",
    "        'disney': 'Disney+ Hotstar',\n",
    "        'amazon prime': 'Amazon Prime',\n",
    "        'prime video': 'Amazon Prime',\n",
    "        'bookmyshow': 'BookMyShow',\n",
    "        'pvr': 'PVR Cinemas',\n",
    "        'inox': 'INOX',\n",
    "        'sony liv': 'Sony LIV',\n",
    "        'zee5': 'ZEE5',\n",
    "        'voot': 'Voot',\n",
    "        \n",
    "        # Travel\n",
    "        'makemytrip': 'MakeMyTrip',\n",
    "        'mmt': 'MakeMyTrip',\n",
    "        'oyo': 'OYO Rooms',\n",
    "        'uber': 'Uber',\n",
    "        'ola': 'Ola Cabs',\n",
    "        'rapido': 'Rapido',\n",
    "        'goibibo': 'Goibibo',\n",
    "        'cleartrip': 'Cleartrip',\n",
    "        'easemytrip': 'EaseMyTrip',\n",
    "        'yatra': 'Yatra',\n",
    "        \n",
    "        # Utilities\n",
    "        'tata play': 'Tata Play',\n",
    "        'dish tv': 'Dish TV',\n",
    "        'd2h': 'D2H',\n",
    "        \n",
    "        # Fuel\n",
    "        'shell': 'Shell',\n",
    "        'reliance petroleum': 'Reliance Petroleum',\n",
    "        'essar': 'Essar Oil',\n",
    "        \n",
    "        # Healthcare\n",
    "        'apollo': 'Apollo Pharmacy',\n",
    "        'medplus': 'MedPlus',\n",
    "        'pharmeasy': 'PharmEasy',\n",
    "        '1mg': '1MG',\n",
    "        'netmeds': 'Netmeds',\n",
    "        'wellness forever': 'Wellness Forever',\n",
    "        'frank ross': 'Frank Ross',\n",
    "        'guardian pharmacy': 'Guardian Pharmacy',\n",
    "        \n",
    "        # Fashion & Beauty\n",
    "        'lifestyle': 'Lifestyle',\n",
    "        'westside': 'Westside',\n",
    "        'shoppers stop': 'Shoppers Stop',\n",
    "        'central': 'Central',\n",
    "        'pantaloons': 'Pantaloons',\n",
    "        'max fashion': 'Max Fashion',\n",
    "        'h&m': 'H&M',\n",
    "        'zara': 'Zara',\n",
    "        'marks spencer': 'Marks & Spencer',\n",
    "        'fabindia': 'FabIndia',\n",
    "        'biba': 'Biba',\n",
    "        'w for woman': 'W',\n",
    "        \n",
    "        # Electronics\n",
    "        'croma': 'Croma',\n",
    "        'reliance digital': 'Reliance Digital',\n",
    "        'vijay sales': 'Vijay Sales',\n",
    "        'apple': 'Apple',\n",
    "        'samsung': 'Samsung',\n",
    "        'oneplus': 'OnePlus',\n",
    "        'mi store': 'Mi Store',\n",
    "        'lenovo': 'Lenovo',\n",
    "        'dell': 'Dell',\n",
    "        'hp world': 'HP'\n",
    "    }\n",
    "    \n",
    "    # Common prefixes to remove\n",
    "    prefixes_to_remove = [\n",
    "        'payment to', 'payment from', 'paid to', 'upi', 'transfer to', 'transfer from',\n",
    "        'payment at', 'payment for', 'purchase from', 'purchase at', 'bill payment',\n",
    "        'recharge', 'subscription', 'order from', 'order at'\n",
    "    ]\n",
    "    \n",
    "    # Common suffixes to remove\n",
    "    suffixes_to_remove = [\n",
    "        'pvt ltd', 'private limited', 'limited', 'ltd', 'india', 'retail', 'online',\n",
    "        'services', 'service', 'solutions', 'solution', 'technologies', 'technology',\n",
    "        'payments', 'payment', 'store', 'stores', 'shop', 'shopping', 'enterprise',\n",
    "        'enterprises', 'corporation', 'corp', 'inc', 'llp', 'company', 'co', 'private',\n",
    "        'pvt', 'order', 'delivery', 'bill', 'recharge', 'subscription'\n",
    "    ]\n",
    "    \n",
    "    # Remove prefixes\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if name.startswith(prefix):\n",
    "            name = name[len(prefix):].strip()\n",
    "    \n",
    "    # Remove suffixes\n",
    "    for suffix in suffixes_to_remove:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[:-len(suffix)].strip()\n",
    "    \n",
    "    # Check mappings\n",
    "    for key, value in merchant_mappings.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    \n",
    "    # If no mapping found, just capitalize words\n",
    "    return ' '.join(word.capitalize() for word in name.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26edb8b-bddc-4676-9d6c-ffee65ed5ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure merchant_standardized and merchant_extracted exist\n",
    "if 'merchant_standardized' not in sample_df_clean.columns:\n",
    "    sample_df_clean['merchant_standardized'] = sample_df_clean['merchant'].apply(standardize_merchant_name)\n",
    "\n",
    "if 'merchant_extracted' not in sample_df_clean.columns:\n",
    "    # Fallback: use raw merchant if no separate extraction logic\n",
    "    sample_df_clean['merchant_extracted'] = sample_df_clean['merchant']\n",
    "\n",
    "# Find merchants with multiple name variants\n",
    "variant_counts = {}\n",
    "for std_name, group in sample_df_clean.groupby('merchant_standardized'):\n",
    "    variants = group['merchant_extracted'].dropna().unique()\n",
    "    if len(variants) > 1:  # Only include merchants with multiple variants\n",
    "        variant_counts[std_name] = {\n",
    "            'variant_count': len(variants),\n",
    "            'variants': variants,\n",
    "            'record_count': len(group)\n",
    "        }\n",
    "\n",
    "# Sort by number of variants\n",
    "top_variants = sorted(variant_counts.items(), key=lambda x: x[1]['variant_count'], reverse=True)\n",
    "\n",
    "print(\"Merchants with Most Name Variations:\")\n",
    "print(f\"{'Standardized Name':<20} {'Variants':<10} {'Records':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for std_name, data in top_variants[:10]:\n",
    "    print(f\"{std_name[:20]:<20} {data['variant_count']:<10} {data['record_count']:<12,}\")\n",
    "    example_variants = ', '.join([str(v) for v in list(data['variants'])[:3]])\n",
    "    print(f\"  Example variants: {example_variants}{'...' if len(data['variants']) > 3 else ''}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffecebef-9a89-4061-84db-91f2b2fbcd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply merchant name standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a595f0-7fcb-4a40-b37c-c97e1e7a0153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, extract merchant names from JSON format\n",
    "print(\"1. Extracting merchant names from JSON...\")\n",
    "sample_df_clean['merchant_extracted'] = sample_df_clean['merchant'].apply(lambda x: re.search(r'\"name\":\"([^\"]+)\"', str(x)).group(1) if re.search(r'\"name\":\"([^\"]+)\"', str(x)) else x)\n",
    "\n",
    "# Then apply standardization\n",
    "print(\"2. Standardizing merchant names...\")\n",
    "sample_df_clean['merchant_standardized'] = sample_df_clean['merchant_extracted'].apply(standardize_merchant_name)\n",
    "\n",
    "# Remove rows where standardization returned None (banks and competitors)\n",
    "print(\"3. Filtering out banks and competitors...\")\n",
    "sample_df_clean = sample_df_clean[sample_df_clean['merchant_standardized'].notna()].copy()\n",
    "\n",
    "# Calculate reduction in unique merchants\n",
    "before_count = sample_df_clean['merchant_extracted'].nunique()\n",
    "after_count = sample_df_clean['merchant_standardized'].nunique()\n",
    "reduction = before_count - after_count\n",
    "reduction_pct = (reduction / before_count * 100) if before_count > 0 else 0\n",
    "\n",
    "print(\"\\nStandardization Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Unique merchants before: {before_count:,}\")\n",
    "print(f\"Unique merchants after: {after_count:,}\")\n",
    "print(f\"Reduction: {reduction:,} ({reduction_pct:.1f}%)\")\n",
    "\n",
    "# Show top standardized merchants by record count\n",
    "print(\"\\nTop 50 Merchants by Record Count:\")\n",
    "print(\"-\" * 50)\n",
    "top_by_records = sample_df_clean['merchant_standardized'].value_counts().head(50)\n",
    "for merchant, count in top_by_records.items():\n",
    "    print(f\"- {merchant}: {count:,} records\")\n",
    "\n",
    "# Show top standardized merchants by user count\n",
    "print(\"\\nTop 50 Merchants by Unique Users:\")\n",
    "print(\"-\" * 50)\n",
    "top_by_users = sample_df_clean.groupby('merchant_standardized')['user_id'].nunique().sort_values(ascending=False).head(50)\n",
    "for merchant, count in top_by_users.items():\n",
    "    print(f\"- {merchant}: {count:,} users\")\n",
    "\n",
    "# Show merchants with most name variations\n",
    "print(\"\\nMerchants with Most Name Variations:\")\n",
    "print(\"-\" * 50)\n",
    "variant_counts = sample_df_clean.groupby('merchant_standardized')['merchant_extracted'].nunique().sort_values(ascending=False).head(10)\n",
    "for merchant, count in variant_counts.items():\n",
    "    variants = sample_df_clean[sample_df_clean['merchant_standardized'] == merchant]['merchant_extracted'].unique()[:3]\n",
    "    print(f\"\\n{merchant}: {count} variants\")\n",
    "    print(f\"Example variants: {', '.join(variants)}{'...' if len(variants) > 3 else ''}\")\n",
    "\n",
    "# Create a dataframe with top merchants data for further analysis\n",
    "top_merchants_df = sample_df_clean.groupby('merchant_standardized').agg({\n",
    "    'user_id': 'nunique',\n",
    "    'total_txns': 'sum',\n",
    "    'total_spend': 'sum',\n",
    "    'merchant_extracted': lambda x: len(set(x))  # Number of variations\n",
    "}).reset_index()\n",
    "\n",
    "top_merchants_df.columns = ['merchant', 'unique_users', 'total_transactions', 'total_spend', 'name_variations']\n",
    "top_merchants_df['avg_spend_per_user'] = top_merchants_df['total_spend'] / top_merchants_df['unique_users']\n",
    "top_merchants_df['avg_txn_value'] = top_merchants_df['total_spend'] / top_merchants_df['total_transactions']\n",
    "top_merchants_df.sort_values('unique_users', ascending=False, inplace=True)\n",
    "\n",
    "# Save top merchants data and standardized data\n",
    "volume_path = \"/Volumes/jupiter/temp/temp/\"\n",
    "\n",
    "# Save top merchants data and standardized data to the Volume\n",
    "top_merchants_df.to_csv(f\"{volume_path}top_merchants_data_backfill.csv\", index=False)\n",
    "sample_df_clean.to_csv(f\"{volume_path}data-standardized_backfill.csv\", index=False)\n",
    "\n",
    "print(\"\\nData saved to Databricks Volume:\")\n",
    "print(f\"- {volume_path}top_merchants_data.csv: Top merchants analysis\")\n",
    "print(f\"- {volume_path}data-standardized.csv: Full standardized dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26683b3-b0cc-43b5-bad4-7ec26ff46add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save top merchants to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d7cfd8-3dac-4dcc-ac0a-4624298afb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"/Volumes/jupiter/temp/temp/top_merchants_data_backfill.csv\"\n",
    "top_merchants_df = pd.read_csv(csv_path)\n",
    "print(f\"Shape: {top_merchants_df.shape}\")\n",
    "print(f\"Columns: {list(top_merchants_df.columns)}\")\n",
    "display(top_merchants_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f255f61-a2bc-4097-928f-8ab4df5a81f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save standardised data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc663c5-e824-41cc-98f1-d6a80a7c7c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"/Volumes/jupiter/temp/temp/data-standardized_backfill.csv\"\n",
    "stand_data_df = pd.read_csv(csv_path)\n",
    "print(f\"Shape: {stand_data_df.shape}\")\n",
    "print(f\"Columns: {list(stand_data_df.columns)}\")\n",
    "display(stand_data_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac57771-7e66-445a-9c4a-aa711a48d85d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze impact of merchant standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171e5f4e-f1c8-418b-913b-a30f324407bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Before standardization metrics\n",
    "before_merchants = sample_df_clean['merchant_extracted'].nunique()\n",
    "before_stats = sample_df_clean.groupby('merchant_extracted').agg({\n",
    "    'user_id': 'nunique',\n",
    "    'total_txns': 'sum',\n",
    "    'total_spend': 'sum'\n",
    "}).sort_values('user_id', ascending=False)\n",
    "\n",
    "# After standardization metrics\n",
    "after_merchants = sample_df_clean['merchant_standardized'].nunique()\n",
    "after_stats = sample_df_clean.groupby('merchant_standardized').agg({\n",
    "    'user_id': 'nunique',\n",
    "    'total_txns': 'sum',\n",
    "    'total_spend': 'sum'\n",
    "}).sort_values('user_id', ascending=False)\n",
    "\n",
    "# Print overall impact\n",
    "print(\"\\nüìà Overall Impact:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Unique merchants before standardization: {before_merchants:,}\")\n",
    "print(f\"Unique merchants after standardization:  {after_merchants:,}\")\n",
    "print(f\"Reduction in merchant variations: {before_merchants - after_merchants:,} ({(before_merchants - after_merchants)/before_merchants:.1%})\")\n",
    "\n",
    "# Print top 20 merchants by user count\n",
    "print(\"\\nüë• Top 20 Merchants by User Count:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Merchant':<30} {'Users':>10} {'Transactions':>15} {'Total Spend':>20}\")\n",
    "print(\"-\" * 80)\n",
    "for merchant, stats in after_stats.head(20).iterrows():\n",
    "    print(f\"{merchant:<30} {stats['user_id']:>10,} {stats['total_txns']:>15,} {stats['total_spend']:>20,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2bc9bbf-ba60-4a46-9f7c-63a7c92b1866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enhanced Merchant Categorization (NPCI Standards + Pattern Matching)\n",
    "\n",
    "We implement a hybrid merchant categorization approach that combines:\n",
    "\n",
    "1. **MCC Code-Based Categorization** (Primary)\n",
    "   - Uses NPCI RuPay Merchant Category Codes (June 2025)\n",
    "   - Covers both POS and ECOM channels\n",
    "   - Prioritizes high-transacting categories\n",
    "\n",
    "2. **Pattern-Based Categorization** (Secondary)\n",
    "   - Extensive keyword matching for each category\n",
    "   - Handles merchants without MCC codes\n",
    "   - Reduces 'Others' category significantly\n",
    "\n",
    "3. **Fallback to Jupiter Defined Categories** (Tertiary)\n",
    "   - Uses columns from `fact-mm-transactions` such as `jupiter-coarsegrain-category` , `usercategory` and `appcategory`\n",
    "   - Handles rows which don't have MCC-Code or Pattern defined\n",
    "\n",
    "**Business Categories:**\n",
    "1. **Food & Dining** - Restaurants, cafes, food delivery\n",
    "2. **Grocery & Supermarkets** - Grocery stores, supermarkets, convenience stores\n",
    "3. **E-Commerce & Retail** - Online shopping, retail stores\n",
    "4. **Financial Services** - Banks, payment apps, insurance\n",
    "5. **Fuel & Transportation** - Petrol, ride-hailing, travel\n",
    "6. **Entertainment & Media** - Streaming, movies, gaming\n",
    "7. **Utilities & Services** - Telecom, electricity, bills\n",
    "8. **Healthcare & Wellness** - Hospitals, pharmacies, fitness\n",
    "9. **Shopping & Fashion** - Clothing, accessories, beauty\n",
    "10. **Education & Learning** - Schools, courses, training\n",
    "11. **Travel & Hospitality** - Hotels, airlines, tourism\n",
    "12. **Technology & Electronics** - Gadgets, computers, software\n",
    "13. **Others** - Unclassified merchants\n",
    "\n",
    "This enhanced categorization provides better granularity and accuracy for merchant analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec50063-827a-476d-8092-78dd71d28750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "# ================================\n",
    "# ‚ö° Initialize Parallelization\n",
    "# ================================\n",
    "pandarallel.initialize(\n",
    "    nb_workers=min(multiprocessing.cpu_count()-1, 8),\n",
    "    progress_bar=True,\n",
    "    verbose=1\n",
    ")\n",
    "tqdm.pandas()\n",
    "\n",
    "# ================================\n",
    "# üìå MCC-based Categorization\n",
    "# ================================\n",
    "def categorize_merchant_by_mcc(mcc_code):\n",
    "    \"\"\"Categorize merchants using NPCI MCC standards. Handles int, float64, str, JSON.\"\"\"\n",
    "    if pd.isna(mcc_code):\n",
    "        return None\n",
    "    \n",
    "    # Normalize MCC string\n",
    "    mcc_str = str(mcc_code).strip()\n",
    "    \n",
    "    # Extract if JSON-like\n",
    "    mcc_match = re.search(r'\"mccCode\":\"(\\d+)\"', mcc_str)\n",
    "    if mcc_match:\n",
    "        mcc_str = mcc_match.group(1)\n",
    "    \n",
    "    try:\n",
    "        mcc = int(float(mcc_str))  # robust parsing for 5812, 5812.0, \"05812\"\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "    # === Category mapping ===\n",
    "    if mcc in [5811, 5812, 5813, 5814, 5462, 5499, 5921]:\n",
    "        return \"Food & Dining\"\n",
    "    elif mcc in [5411, 5422, 5441, 5451, 5310]:\n",
    "        return \"Grocery & Supermarkets\"\n",
    "    elif mcc in [5311, 5399, 5999, 5945, 5734, 5964, 5969, 5970, 5309, 5732, 5722, 4215]:\n",
    "        return \"E-Commerce & Retail\"\n",
    "    elif mcc in [5541, 5542, 4111, 4112, 4121, 4131, 4784, 4789, 7523, 5983]:\n",
    "        return \"Fuel & Transportation\"\n",
    "    elif mcc in [7832, 7922, 7841, 7829, 7932, 7933, 7941, 7991, 7994, 7995,\n",
    "                 7996, 7999, 5815, 5816, 5817, 5818, 4722]:\n",
    "        return \"Entertainment & Media\"\n",
    "    elif mcc in [4814, 4815, 4821, 4899, 4900, 6513, 7299, 7311, 7399, 8111, \n",
    "                 8999, 9399, 9311, 7349]:\n",
    "        return \"Utilities & Services\"\n",
    "    elif mcc in [8062, 8071, 8099, 5912, 5975, 5976, 5977, 8011, 8021, 8031,\n",
    "                 8041, 8042, 8043, 8049, 8050, 5047]:\n",
    "        return \"Healthcare & Wellness\"\n",
    "    elif mcc in [5611, 5621, 5631, 5641, 5651, 5655, 5661, 5681, 5691, 5697,\n",
    "                 5698, 5699, 5931, 5944, 5949, 5950, 5947]:\n",
    "        return \"Shopping & Fashion\"\n",
    "    elif mcc in [8211, 8220, 8241, 8244, 8249, 8299]:\n",
    "        return \"Education & Learning\"\n",
    "    elif mcc in [6012, 6540, 6300]:\n",
    "        return \"Financial Services\"\n",
    "    elif mcc in [7011]:\n",
    "        return \"Travel & Hospitality\"\n",
    "    elif mcc in [5511]:\n",
    "        return \"Automotive & Dealerships\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ================================\n",
    "# üìå Pattern-based Categorization\n",
    "# ================================\n",
    "def categorize_merchant_by_pattern(merchant_name):\n",
    "    \"\"\"Categorize merchants using pattern matching.\"\"\"\n",
    "    if not isinstance(merchant_name, str):\n",
    "        return None\n",
    "    merchant_lower = merchant_name.lower()\n",
    "    patterns = {\n",
    "        \"Food & Dining\": [\"restaurant\",\"food\",\"swiggy\",\"zomato\",\"pizza\",\"burger\",\"cafe\",\"dining\",\"hotel\",\"bar\",\"pub\",\"dhaba\"],\n",
    "        \"Grocery & Supermarkets\": [\"grocery\",\"supermarket\",\"mart\",\"zepto\",\"blinkit\",\"bigbasket\",\"kirana\",\"provision\",\"store\",\"vegetable\",\"fruit\"],\n",
    "        \"E-Commerce & Retail\": [\"amazon\",\"flipkart\",\"myntra\",\"shopping\",\"retail\",\"store\",\"mall\",\"shop\",\"outlet\",\"ecommerce\",\"online\"],\n",
    "        \"Fuel & Transportation\": [\"petrol\",\"fuel\",\"uber\",\"ola\",\"taxi\",\"transport\",\"ride\",\"cab\",\"auto\",\"bus\",\"train\",\"metro\",\"rail\"],\n",
    "        \"Entertainment & Media\": [\"netflix\",\"prime\",\"hotstar\",\"movie\",\"cinema\",\"theatre\",\"concert\",\"ticket\",\"streaming\",\"subscription\",\"music\"],\n",
    "        \"Utilities & Services\": [\"recharge\",\"bill\",\"utility\",\"airtel\",\"jio\",\"vi\",\"bsnl\",\"telecom\",\"mobile\",\"wifi\",\"electricity\",\"power\"],\n",
    "        \"Healthcare & Wellness\": [\"hospital\",\"pharmacy\",\"medical\",\"apollo\",\"doctor\",\"clinic\",\"checkup\",\"medicine\",\"drug\",\"surgery\",\"gym\",\"fitness\"],\n",
    "        \"Shopping & Fashion\": [\"fashion\",\"clothing\",\"apparel\",\"lifestyle\",\"accessories\",\"wear\",\"jeans\",\"saree\",\"kurta\",\"lehenga\",\"footwear\",\"jewelry\"],\n",
    "        \"Education & Learning\": [\"education\",\"school\",\"college\",\"university\",\"academy\",\"coaching\",\"tuition\",\"tutorial\",\"class\",\"course\",\"training\"]\n",
    "    }\n",
    "    for category, keywords in patterns.items():\n",
    "        if any(k in merchant_lower for k in keywords):\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "# ================================\n",
    "# üìå Explicit Merchant Mapping\n",
    "# ================================\n",
    "merchant_category_mapping = {\n",
    "    \"Swiggy\": \"Food & Dining\", \"Zomato\": \"Food & Dining\", \"Dominos Pizza\": \"Food & Dining\",\n",
    "    \"Amazon\": \"E-Commerce & Retail\", \"Flipkart\": \"E-Commerce & Retail\", \"Myntra\": \"E-Commerce & Retail\",\n",
    "    \"DMart\": \"Grocery & Supermarkets\", \"Blinkit\": \"Grocery & Supermarkets\", \"BigBasket\": \"Grocery & Supermarkets\",\n",
    "    \"Netflix\": \"Entertainment & Media\", \"Disney+ Hotstar\": \"Entertainment & Media\", \"BookMyShow\": \"Entertainment & Media\",\n",
    "    \"Jio\": \"Utilities & Services\", \"Airtel\": \"Utilities & Services\", \"Apollo Pharmacy\": \"Healthcare & Wellness\",\n",
    "    \"Lifestyle\": \"Shopping & Fashion\", \"H&M\": \"Shopping & Fashion\", \"Zara\": \"Shopping & Fashion\"\n",
    "}\n",
    "\n",
    "# ================================\n",
    "# üìå Hybrid Categorization\n",
    "# ================================\n",
    "def categorize_merchant_hybrid(row):\n",
    "    \"\"\"Hybrid categorization with MCC, explicit mapping, pattern, and fallbacks.\"\"\"\n",
    "    merchant_name = row.get(\"merchant_standardized\", None)\n",
    "    \n",
    "    # 1. Explicit\n",
    "    if merchant_name in merchant_category_mapping:\n",
    "        return merchant_category_mapping[merchant_name]\n",
    "    \n",
    "    # 2. MCC\n",
    "    if \"mcccode\" in row and pd.notna(row[\"mcccode\"]):\n",
    "        mcc_category = categorize_merchant_by_mcc(row[\"mcccode\"])\n",
    "        if mcc_category:\n",
    "            return mcc_category\n",
    "    \n",
    "    # 3. Pattern\n",
    "    pattern_category = categorize_merchant_by_pattern(merchant_name)\n",
    "    if pattern_category:\n",
    "        return pattern_category\n",
    "    \n",
    "    # 4. Fallbacks: Jupiter, usercategory, appcategory (if present)\n",
    "    for col in [\"jupiter_coarsegrain_category\", \"usercategory\", \"appcategory\"]:\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            cat_val = str(row[col]).strip().upper()\n",
    "            if \"FOOD\" in cat_val: return \"Food & Dining\"\n",
    "            if \"GROCERY\" in cat_val or \"SUPERMARKET\" in cat_val: return \"Grocery & Supermarkets\"\n",
    "            if \"SHOPPING\" in cat_val or \"RETAIL\" in cat_val or \"ECOM\" in cat_val: return \"E-Commerce & Retail\"\n",
    "            if \"TRAVEL\" in cat_val or \"FUEL\" in cat_val or \"TRANSPORT\" in cat_val: return \"Fuel & Transportation\"\n",
    "            if \"ENTERTAIN\" in cat_val or \"MOVIE\" in cat_val or \"MEDIA\" in cat_val: return \"Entertainment & Media\"\n",
    "            if \"UTILITY\" in cat_val or \"BILL\" in cat_val or \"SERVICE\" in cat_val: return \"Utilities & Services\"\n",
    "            if \"HEALTH\" in cat_val or \"MEDICAL\" in cat_val or \"PHARM\" in cat_val: return \"Healthcare & Wellness\"\n",
    "            if \"FASHION\" in cat_val or \"CLOTH\" in cat_val or \"APPAREL\" in cat_val: return \"Shopping & Fashion\"\n",
    "            if \"EDUCATION\" in cat_val or \"LEARN\" in cat_val or \"SCHOOL\" in cat_val or \"COLLEGE\" in cat_val: return \"Education & Learning\"\n",
    "    \n",
    "    # 5. Default\n",
    "    return \"Others\"\n",
    "\n",
    "print(\"‚ö° Applying hybrid categorization with pandarallel...\")\n",
    "sample_df_clean[\"merchant_category\"] = sample_df_clean.parallel_apply(categorize_merchant_hybrid, axis=1)\n",
    "\n",
    "# Distribution\n",
    "category_counts = sample_df_clean[\"merchant_category\"].value_counts()\n",
    "print(\"\\nCategory Distribution After Hybrid Categorization:\")\n",
    "for cat, cnt in category_counts.items():\n",
    "    print(f\"{cat:<25} {cnt:>8,} ({cnt/len(sample_df_clean):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3febe51a-a571-4439-985f-20ccab2cabc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# üìä MCC vs Merchant Category Analysis (exclude 0 + null)\n",
    "# ================================\n",
    "\n",
    "# 1. Filter rows with valid MCC codes (not null and not 0)\n",
    "mcc_df = sample_df_clean[\n",
    "    (sample_df_clean[\"mcccode\"].notna()) & (sample_df_clean[\"mcccode\"] != 0)\n",
    "].copy()\n",
    "\n",
    "# 2. Distribution of MCC ‚Üí Merchant Category\n",
    "mcc_category_distribution = (\n",
    "    mcc_df.groupby(\"mcccode\")[\"merchant_category\"]\n",
    "    .value_counts(normalize=False)\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# 3. Percentage of MCC-coded rows labeled as \"Others\"\n",
    "total_mcc_rows = len(mcc_df)\n",
    "others_rows = len(mcc_df[mcc_df[\"merchant_category\"] == \"Others\"])\n",
    "others_pct = (others_rows / total_mcc_rows) * 100\n",
    "\n",
    "print(f\"Total rows with valid MCC code: {total_mcc_rows:,}\")\n",
    "print(f\"Rows with MCC code & category='Others': {others_rows:,} ({others_pct:.2f}%)\")\n",
    "\n",
    "# 4. Show the pivot table (MCC ‚Üí categories)\n",
    "display(mcc_category_distribution.head(20))  # show top 20 for readability\n",
    "\n",
    "# 5. Visualization: Heatmap of MCC vs Categories\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(mcc_category_distribution.T, cmap=\"Blues\", cbar=True)\n",
    "plt.title(\"MCC vs Merchant Category Distribution\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"MCC Code\")\n",
    "plt.ylabel(\"Merchant Category\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Visualization: Focus on 'Others' share by MCC\n",
    "others_share = (\n",
    "    mcc_df.groupby(\"mcccode\")[\"merchant_category\"]\n",
    "    .apply(lambda x: (x == \"Others\").mean())\n",
    "    .reset_index(name=\"others_share\")\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(data=others_share.sort_values(\"others_share\", ascending=False).head(20),\n",
    "            x=\"mcccode\", y=\"others_share\", palette=\"Reds_r\")\n",
    "plt.title(\"Top 20 MCC Codes with Highest 'Others' Share\", fontsize=14, weight=\"bold\")\n",
    "plt.ylabel(\"Share of 'Others'\")\n",
    "plt.xlabel(\"MCC Code\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a24c7ac-4df3-4bc7-8ea1-e67856db703d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RFM Analysis for User-Merchant Pairs\n",
    "\n",
    "**RFM Framework**: Analyze customer behavior patterns using three key dimensions:\n",
    "- **Recency (R)**: How recently did a user transact with a merchant?\n",
    "- **Frequency (F)**: How often does a user transact with a merchant?  \n",
    "- **Monetary (M)**: How much does a user spend with a merchant?\n",
    "\n",
    "**Implementation for 50M+ Records**:\n",
    "- **Optimized Grouping**: Efficient aggregation by user-merchant pairs\n",
    "- **Quintile Scoring**: 1-5 scale for each RFM dimension\n",
    "- **Customer Segmentation**: 8 distinct behavioral segments\n",
    "- **Performance Tracking**: Monitor processing time and memory usage\n",
    "\n",
    "**Customer Segments**:\n",
    "- **Champions** (R5,F5,M5): Best customers - high value, frequent, recent\n",
    "- **Loyal Customers** (R4-5,F4-5,M3-5): Consistent high-value customers\n",
    "- **Potential Loyalists** (R4-5,F2-3,M1-3): Recent customers with growth potential  \n",
    "- **New Customers** (R4-5,F1,M1-2): Recently acquired users\n",
    "- **At Risk** (R1-2,F4-5,M3-5): Valuable customers becoming inactive\n",
    "- **Cannot Lose Them** (R1-2,F1,M4-5): Inactive but historically high-value\n",
    "- **Lost Customers** (R1,F1,M1): Churned low-value customers\n",
    "- **Others**: All other combinations\n",
    "\n",
    "**Output**: User-merchant pair level RFM scores and segments for recommendation engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dfcd5a4-a779-4125-b012-2aac2ba0c550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# -------------------------\n",
    "# 1. Helper: Customer Segmentation\n",
    "# -------------------------\n",
    "def get_customer_segment(row):\n",
    "    \"\"\"Assign customer segments based on RFM scores\"\"\"\n",
    "    r, f, m = row['R_score'], row['F_score'], row['M_score']\n",
    "\n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return 'Champions'\n",
    "    elif r >= 3 and f >= 3 and m >= 3:\n",
    "        return 'Loyal Customers'\n",
    "    elif r >= 4 and f <= 2:\n",
    "        return 'New Customers'\n",
    "    elif r >= 3 and f >= 3 and m <= 2:\n",
    "        return 'Potential Loyalists'\n",
    "    elif r <= 2 and f >= 3:\n",
    "        return 'At Risk'\n",
    "    elif r <= 2 and f <= 2 and m >= 3:\n",
    "        return 'Cannot Lose Them'\n",
    "    elif r <= 2 and f <= 2 and m <= 2:\n",
    "        return 'Lost Customers'\n",
    "    else:\n",
    "        return 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7540144-e2f9-4fec-a6b7-125e3e33f4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. Generic RFM Analysis Function\n",
    "# -------------------------\n",
    "def run_rfm_analysis(df, groupby_cols, analysis_name=\"Merchant-Level\"):\n",
    "    \"\"\"\n",
    "    Run RFM analysis on given grouping (merchant or category)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): input data\n",
    "        groupby_cols (list): grouping cols (['user_id','merchant_standardized'] or ['user_id','merchant_category'])\n",
    "        analysis_name (str): Name of analysis\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä RFM Analysis: {analysis_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Step 1: Pick recency column\n",
    "    date_columns = [\"last_txn_date\", \"first_txn_date\", \"transactiondatetime\"]\n",
    "    available_date_cols = [c for c in date_columns if c in df.columns]\n",
    "    date_col = available_date_cols[0] if available_date_cols else None\n",
    "    current_date = df[date_col].max() if date_col else pd.Timestamp.now()\n",
    "\n",
    "    print(f\"üìÖ Using {date_col if date_col else 'synthetic recency'} as reference\")\n",
    "    print(f\"   Reference date: {current_date}\\n\")\n",
    "\n",
    "        # --- Step 2: Aggregate for R, F, M\n",
    "    rfm = df.groupby(groupby_cols).agg({\n",
    "            'total_txns': 'sum',      # Frequency\n",
    "            'total_spend': 'sum',     # Monetary\n",
    "            date_col if date_col else groupby_cols[0]: 'max' if date_col else 'count'\n",
    "        }).reset_index()\n",
    "\n",
    "    if date_col:\n",
    "            rfm.columns = groupby_cols + ['frequency', 'monetary', 'last_transaction_date']\n",
    "            rfm['recency'] = (current_date - rfm['last_transaction_date']).dt.days\n",
    "    else:\n",
    "            rfm.columns = groupby_cols + ['frequency', 'monetary', 'temp_col']\n",
    "            max_freq = rfm['frequency'].max()\n",
    "            rfm['recency'] = ((max_freq - rfm['frequency']) / max_freq * 365).astype(int)\n",
    "\n",
    "    # --- Step 3: RFM Scores (1‚Äì5)\n",
    "    rfm['R_score'] = pd.qcut(rfm['recency'].rank(method='first'), q=5, labels=[5,4,3,2,1]).astype(int)\n",
    "    rfm['F_score'] = pd.qcut(rfm['frequency'].rank(method='first'), q=5, labels=[1,2,3,4,5]).astype(int)\n",
    "    rfm['M_score'] = pd.qcut(rfm['monetary'].rank(method='first'), q=5, labels=[1,2,3,4,5]).astype(int)\n",
    "    rfm['RFM_score'] = rfm['R_score'].astype(str) + rfm['F_score'].astype(str) + rfm['M_score'].astype(str)\n",
    "\n",
    "    # --- Step 4: Segmentation\n",
    "    rfm['customer_segment'] = rfm.apply(get_customer_segment, axis=1)\n",
    "\n",
    "    processing_time = time.time() - start_time\n",
    "    print(f\"‚úÖ RFM metrics & scores calculated in {processing_time:.2f}s\")\n",
    "    print(f\"   Pairs analyzed: {len(rfm):,}\\n\")\n",
    "\n",
    "    # --- Step 5: Reporting\n",
    "    segment_counts = rfm['customer_segment'].value_counts()\n",
    "    print(\"üë• Customer Segment Distribution:\")\n",
    "    for seg, count in segment_counts.items():\n",
    "        print(f\"   {seg:<20}: {count:>8,} ({count/len(rfm)*100:5.1f}%)\")\n",
    "\n",
    "    print(\"\\nüî¢ RFM Score Distributions:\")\n",
    "    print(\"   Recency :\", dict(rfm['R_score'].value_counts().sort_index()))\n",
    "    print(\"   Frequency:\", dict(rfm['F_score'].value_counts().sort_index()))\n",
    "    print(\"   Monetary :\", dict(rfm['M_score'].value_counts().sort_index()))\n",
    "\n",
    "    return rfm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55f39de-5e79-458b-af61-f5d9db9475fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Run for Merchant-Level and Category-Level\n",
    "# -------------------------\n",
    "merchant_rfm = run_rfm_analysis(sample_df_clean, ['user_id','merchant_standardized'], \"User-Merchant\")\n",
    "category_rfm = run_rfm_analysis(sample_df_clean, ['user_id','merchant_category'], \"User-Category\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Merge Back to Original for Visibility (per row tagging)\n",
    "# -------------------------\n",
    "sample_df_clean = sample_df_clean.merge(\n",
    "    merchant_rfm[['user_id','merchant_standardized','R_score','F_score','M_score','RFM_score','customer_segment']],\n",
    "    on=['user_id','merchant_standardized'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "sample_df_clean = sample_df_clean.merge(\n",
    "    category_rfm[['user_id','merchant_category','R_score','F_score','M_score','RFM_score','customer_segment']],\n",
    "    on=['user_id','merchant_category'],\n",
    "    how='left',\n",
    "    suffixes=('_merchant','_category')\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Final dataframe now includes RFM scores & segments at both Merchant and Category level.\")\n",
    "print(f\"Shape: {sample_df_clean.shape}\")\n",
    "print(sample_df_clean.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80db5f3-1773-43a2-8739-3862d66ec954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sample_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a3a37e-ceb2-47c6-9263-1346835a98c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make plots prettier\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\", font_scale=1.2)\n",
    "\n",
    "# ===============================\n",
    "# 1. Customer Segment Distribution (Merchant-level)\n",
    "# ===============================\n",
    "plt.figure(figsize=(10,6))\n",
    "segment_counts = merchant_rfm['customer_segment'].value_counts().sort_values(ascending=False)\n",
    "sns.barplot(x=segment_counts.index, y=segment_counts.values)\n",
    "plt.title(\"Merchant-Level Customer Segment Distribution\", fontsize=16)\n",
    "plt.ylabel(\"Number of User-Merchant Pairs\")\n",
    "plt.xlabel(\"Customer Segment\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 2. Customer Segment Distribution (Category-level)\n",
    "# ===============================\n",
    "plt.figure(figsize=(10,6))\n",
    "cat_segment_counts = category_rfm['customer_segment'].value_counts().sort_values(ascending=False)\n",
    "sns.barplot(x=cat_segment_counts.index, y=cat_segment_counts.values)\n",
    "plt.title(\"Category-Level Customer Segment Distribution\", fontsize=16)\n",
    "plt.ylabel(\"Number of User-Category Pairs\")\n",
    "plt.xlabel(\"Customer Segment\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 3. RFM Score Heatmap (Merchant-level)\n",
    "# ===============================\n",
    "rfm_heatmap = merchant_rfm.groupby(['R_score','F_score']).size().unstack(fill_value=0)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(rfm_heatmap, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Heatmap of Recency vs Frequency (Merchant-Level)\")\n",
    "plt.ylabel(\"Recency Score\")\n",
    "plt.xlabel(\"Frequency Score\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 4. Top Categories by Champions\n",
    "# ===============================\n",
    "top_champion_cats = category_rfm[category_rfm['customer_segment']==\"Champions\"] \\\n",
    "                        .groupby('merchant_category')['user_id'].nunique() \\\n",
    "                        .sort_values(ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=top_champion_cats.values, y=top_champion_cats.index)\n",
    "plt.title(\"Top 10 Categories by Champion Users\")\n",
    "plt.xlabel(\"Unique Champion Users\")\n",
    "plt.ylabel(\"Merchant Category\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 5. Segment Distribution by Category (stacked bar)\n",
    "# ===============================\n",
    "cat_segment_dist = category_rfm.groupby(['merchant_category','customer_segment']).size().reset_index(name=\"count\")\n",
    "cat_segment_pivot = cat_segment_dist.pivot(index=\"merchant_category\", columns=\"customer_segment\", values=\"count\").fillna(0)\n",
    "\n",
    "cat_segment_pivot_pct = cat_segment_pivot.div(cat_segment_pivot.sum(axis=1), axis=0) * 100\n",
    "cat_segment_pivot_pct.sort_values(\"Champions\", ascending=False, inplace=True)\n",
    "\n",
    "cat_segment_pivot_pct.head(10).plot(kind=\"bar\", stacked=True, figsize=(12,6), colormap=\"tab20\")\n",
    "plt.title(\"Segment Composition of Top 10 Categories\")\n",
    "plt.ylabel(\"Percentage of Users\")\n",
    "plt.xlabel(\"Merchant Category\")\n",
    "plt.legend(title=\"Segment\", bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 6. Average Spend & Transactions by Segment (Merchant-level)\n",
    "# ===============================\n",
    "plt.figure(figsize=(10,6))\n",
    "avg_metrics = merchant_rfm.groupby('customer_segment')[['frequency','monetary']].mean().sort_values('monetary', ascending=False)\n",
    "avg_metrics.plot(kind=\"bar\", figsize=(10,6))\n",
    "plt.title(\"Average Frequency & Monetary by Segment (Merchant-level)\")\n",
    "plt.ylabel(\"Average Value\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 7. Category Engagement Summary\n",
    "# ===============================\n",
    "cat_stats = category_rfm.groupby('merchant_category').agg({\n",
    "    'user_id':'nunique',\n",
    "    'frequency':'sum',\n",
    "    'monetary':'sum'\n",
    "}).sort_values('user_id', ascending=False).head(10)\n",
    "\n",
    "cat_stats['avg_spend_per_user'] = cat_stats['monetary'] / cat_stats['user_id']\n",
    "cat_stats['avg_txns_per_user'] = cat_stats['frequency'] / cat_stats['user_id']\n",
    "\n",
    "print(\"üìä Top 10 Categories Engagement Stats:\")\n",
    "display(cat_stats[['user_id','frequency','monetary','avg_spend_per_user','avg_txns_per_user']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4892fa5e-8860-45ce-b4ed-f937a112aaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"Set2\", font_scale=1.1)\n",
    "\n",
    "# Ensure RFM segmentation columns exist\n",
    "if 'customer_segment' not in sample_df_clean.columns:\n",
    "    print(\"‚ö†Ô∏è Running RFM analysis to add missing segmentation columns...\")\n",
    "\n",
    "    # Calculate RFM scores again (minimal version)\n",
    "    current_date = pd.to_datetime(sample_df_clean['last_txn_date']).max()\n",
    "\n",
    "    rfm_data = sample_df_clean.groupby(['user_id', 'merchant_standardized']).agg({\n",
    "        'total_txns': 'sum',\n",
    "        'total_spend': 'sum',\n",
    "        'last_txn_date': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    rfm_data['recency'] = (current_date - rfm_data['last_txn_date']).dt.days\n",
    "\n",
    "    # R, F, M scores\n",
    "    rfm_data['R_score'] = pd.qcut(rfm_data['recency'].rank(method='first'), 5, labels=[5,4,3,2,1]).astype(int)\n",
    "    rfm_data['F_score'] = pd.qcut(rfm_data['total_txns'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)\n",
    "    rfm_data['M_score'] = pd.qcut(rfm_data['total_spend'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "    rfm_data['RFM_score'] = (\n",
    "        rfm_data['R_score'].astype(str) +\n",
    "        rfm_data['F_score'].astype(str) +\n",
    "        rfm_data['M_score'].astype(str)\n",
    "    )\n",
    "\n",
    "    def get_segment(row):\n",
    "        r, f, m = row['R_score'], row['F_score'], row['M_score']\n",
    "        if r >= 4 and f >= 4 and m >= 4: return 'Champions'\n",
    "        elif r >= 3 and f >= 3 and m >= 3: return 'Loyal Customers'\n",
    "        elif r >= 4 and f <= 2: return 'New Customers'\n",
    "        elif r >= 3 and f >= 3 and m <= 2: return 'Potential Loyalists'\n",
    "        elif r <= 2 and f >= 3: return 'At Risk'\n",
    "        elif r <= 2 and f <= 2 and m >= 3: return 'Cannot Lose Them'\n",
    "        elif r <= 2 and f <= 2 and m <= 2: return 'Lost Customers'\n",
    "        else: return 'Others'\n",
    "\n",
    "    rfm_data['customer_segment'] = rfm_data.apply(get_segment, axis=1)\n",
    "\n",
    "    # Merge back into sample_df_clean\n",
    "    sample_df_clean = sample_df_clean.merge(\n",
    "        rfm_data[['user_id','merchant_standardized','R_score','F_score','M_score','RFM_score','customer_segment']],\n",
    "        on=['user_id','merchant_standardized'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Added RFM scores & segments to sample_df_clean\")\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 1. Top 30 Merchants by User Count\n",
    "# ===================================\n",
    "top_merchants = (\n",
    "    sample_df_clean.groupby(\"merchant_standardized\")[\"user_id\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(30)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=top_merchants.values, y=top_merchants.index)\n",
    "plt.title(\"üèÜ Top 30 Merchants by Unique Users\")\n",
    "plt.xlabel(\"Unique Users\")\n",
    "plt.ylabel(\"Merchant\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 2. Pie Chart of Categories (by unique users)\n",
    "# ===================================\n",
    "category_users = (\n",
    "    sample_df_clean.groupby(\"merchant_category\")[\"user_id\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(category_users.values, labels=category_users.index, autopct=\"%1.1f%%\", startangle=140)\n",
    "plt.title(\"üë• Users Interacting in Categories\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 3. Segment Distribution within Each Category\n",
    "# ===================================\n",
    "cat_seg = (\n",
    "    sample_df_clean.groupby([\"merchant_category\",\"customer_segment\"])[\"user_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(data=cat_seg, x=\"merchant_category\", y=\"user_id\", hue=\"customer_segment\")\n",
    "plt.title(\"üìä Segment Distribution Across Categories\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Unique Users\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 4. Top Categories by Champions\n",
    "# ===================================\n",
    "top_champion_cats = (\n",
    "    sample_df_clean[sample_df_clean[\"customer_segment\"]==\"Champions\"]\n",
    "    .groupby(\"merchant_category\")[\"user_id\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=top_champion_cats.values, y=top_champion_cats.index)\n",
    "plt.title(\"üèÜ Top 10 Categories by Champion Users\")\n",
    "plt.xlabel(\"Champion Users\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 5. Average Spend per Segment\n",
    "# ===================================\n",
    "avg_spend = (\n",
    "    sample_df_clean.groupby(\"customer_segment\")[\"total_spend\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=avg_spend.index, y=avg_spend.values)\n",
    "plt.title(\"üí∞ Average Spend by Segment\")\n",
    "plt.ylabel(\"Avg Spend (‚Çπ)\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 6. Heatmap of RFM Scores (Recency vs Frequency)\n",
    "# ===================================\n",
    "rfm_heatmap = (\n",
    "    sample_df_clean.groupby([\"R_score\",\"F_score\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(rfm_heatmap, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.title(\"üìà Heatmap: Recency vs Frequency (All Merchants)\")\n",
    "plt.xlabel(\"Frequency Score\")\n",
    "plt.ylabel(\"Recency Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2d30f0-3a2c-44a0-9676-c2a81695dff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üîë Extra RFM Metrics & Enrichment\n",
    "# =====================================\n",
    "\n",
    "print(\"üîß Adding extra RFM metrics...\")\n",
    "\n",
    "# --------------------------\n",
    "# 1. Weighted RFM Score\n",
    "# --------------------------\n",
    "# Adjust weights depending on business need (recency usually matters more)\n",
    "merchant_rfm['RFM_weighted'] = (\n",
    "    merchant_rfm['R_score']*0.4 + \n",
    "    merchant_rfm['F_score']*0.3 + \n",
    "    merchant_rfm['M_score']*0.3\n",
    ").round(2)\n",
    "\n",
    "category_rfm['RFM_weighted'] = (\n",
    "    category_rfm['R_score']*0.4 + \n",
    "    category_rfm['F_score']*0.3 + \n",
    "    category_rfm['M_score']*0.3\n",
    ").round(2)\n",
    "\n",
    "# --------------------------\n",
    "# 2. Per-user Percentiles (normalize within user)\n",
    "# --------------------------\n",
    "# Merchant-level\n",
    "merchant_rfm['F_percentile'] = merchant_rfm.groupby('user_id')['frequency'].rank(pct=True)\n",
    "merchant_rfm['M_percentile'] = merchant_rfm.groupby('user_id')['monetary'].rank(pct=True)\n",
    "\n",
    "# Category-level\n",
    "category_rfm['F_percentile'] = category_rfm.groupby('user_id')['frequency'].rank(pct=True)\n",
    "category_rfm['M_percentile'] = category_rfm.groupby('user_id')['monetary'].rank(pct=True)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Engagement Index\n",
    "# --------------------------\n",
    "merchant_rfm['engagement_index'] = (\n",
    "    merchant_rfm['F_percentile']*0.5 + merchant_rfm['M_percentile']*0.5\n",
    ").round(3)\n",
    "\n",
    "category_rfm['engagement_index'] = (\n",
    "    category_rfm['F_percentile']*0.5 + category_rfm['M_percentile']*0.5\n",
    ").round(3)\n",
    "\n",
    "print(\"‚úÖ Extra RFM metrics added: RFM_weighted, F_percentile, M_percentile, engagement_index\")\n",
    "\n",
    "# --------------------------\n",
    "# 4. Merge back into sample_df_clean\n",
    "# --------------------------\n",
    "# Merchant-level enrich\n",
    "sample_df_clean = sample_df_clean.merge(\n",
    "    merchant_rfm[['user_id','merchant_standardized','R_score','F_score','M_score',\n",
    "                  'RFM_score','RFM_weighted','customer_segment',\n",
    "                  'F_percentile','M_percentile','engagement_index']],\n",
    "    how='left',\n",
    "    left_on=['user_id','merchant_standardized'],\n",
    "    right_on=['user_id','merchant_standardized']\n",
    ")\n",
    "\n",
    "# Category-level enrich\n",
    "sample_df_clean = sample_df_clean.merge(\n",
    "    category_rfm[['user_id','merchant_category','R_score','F_score','M_score',\n",
    "                  'RFM_score','RFM_weighted','customer_segment',\n",
    "                  'F_percentile','M_percentile','engagement_index']],\n",
    "    how='left',\n",
    "    left_on=['user_id','merchant_category'],\n",
    "    right_on=['user_id','merchant_category'],\n",
    "    suffixes=('_merchant','_category')\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RFM enrichment merged back into sample_df_clean\")\n",
    "print(f\"üìä Final enriched shape: {sample_df_clean.shape}\")\n",
    "display(sample_df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd4e4af-27d0-4317-9dae-6b35b18e86b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert all boolean columns in one go\n",
    "bool_cols = sample_df_clean.select_dtypes(include=['bool']).columns\n",
    "sample_df_clean[bool_cols] = sample_df_clean[bool_cols].astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93995d1-e59f-45d8-a643-d5c22dbe23e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(sample_df_clean)\n",
    "\n",
    "# Use the Databricks display() command\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0724c8-0b20-40a2-a654-c5f084092ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sample_df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb11032-908a-4f6f-a0ce-e6b5bddff960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Visualization of Enriched RFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8734bb-a8fe-43b6-9860-3c8a5ab78139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Distribution of Weighted RFM Scores\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(sample_df_clean['RFM_weighted_merchant'].dropna(), bins=20, kde=True)\n",
    "plt.title(\"Distribution of Weighted RFM Scores (Merchant-level)\")\n",
    "plt.xlabel(\"RFM Weighted Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(sample_df_clean['RFM_weighted_category'].dropna(), bins=20, kde=True, color=\"orange\")\n",
    "plt.title(\"Distribution of Weighted RFM Scores (Category-level)\")\n",
    "plt.xlabel(\"RFM Weighted Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6b10da-55a5-4681-bdd5-a4e78b394eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Category Segment Distribution\n",
    "# -------------------------------\n",
    "cat_seg = (\n",
    "    sample_df_clean.groupby([\"merchant_category\",\"customer_segment_category\"])[\"user_id\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(data=cat_seg, x=\"merchant_category\", y=\"user_id\", hue=\"customer_segment_category\")\n",
    "plt.title(\"User Distribution Across Segments per Category\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Unique Users\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ebaf7f7-c79e-4fa1-83f9-724cf98d91ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# üíæ Save Enriched RFM Data (Training Set)\n",
    "# ================================\n",
    "\n",
    "output_path = \"/Volumes/jupiter/temp/temp/rfm_analysis_test.csv\"\n",
    "\n",
    "sample_df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Enriched RFM training dataset saved to: {output_path}\")\n",
    "print(f\"   Shape: {sample_df_clean.shape}\")\n",
    "print(f\"   Columns: {list(sample_df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03041368-0e7b-4362-99e2-9a57ed0aa181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# 1. Convert the DataFrame to a CSV string.\n",
    "# `index=False` prevents pandas from writing row indices into the CSV.\n",
    "csv_string = sample_df_clean.to_csv(index=False)\n",
    "\n",
    "# 2. Encode the CSV string into base64.\n",
    "# This is necessary to embed the file data directly into the HTML link.\n",
    "b64 = base64.b64encode(csv_string.encode()).decode()\n",
    "\n",
    "# 3. Define the filename for the download.\n",
    "file_name = \"rfm_test.csv\"\n",
    "\n",
    "# 4. Create an HTML anchor tag (`<a>`) with the download link.\n",
    "# The `href` attribute contains the base64-encoded data with the correct MIME type.\n",
    "# The `download` attribute tells the browser to download the file with the specified name.\n",
    "# --- FIX IS ON THIS LINE ---\n",
    "download_link = f'<a href=\"data:text/csv;base64,{b64}\" download=\"{file_name}\">Click here to download {file_name}</a>'\n",
    "\n",
    "# 5. Display the link in the notebook output.\n",
    "# When you run this cell, a clickable link will appear.\n",
    "display(HTML(download_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f779b5-7c72-4ad8-8785-6769ab669abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Backfill Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5ea32e3-4edc-407e-bca0-44561cfe73dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde17362-890f-4bdc-a23e-4df4dd47a92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandarallel import pandarallel\n",
    "import multiprocessing\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Parameters\n",
    "K = 3  # Top-K merchants/categories for evaluation\n",
    "pandarallel.initialize(nb_workers=min(multiprocessing.cpu_count()-1, 8), progress_bar=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d09cac-db3a-4259-9e95-44a697eb4715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_spark = spark.read.csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_train.csv\", header=True, inferSchema=True)\n",
    "# train_df = train_spark.toPandas()  # convert only if memory allows\n",
    "\n",
    "# test_spark = spark.read.csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_test.csv\", header=True, inferSchema=True)\n",
    "# test_df = test_spark.toPandas()  # convert only if memory allows\n",
    "\n",
    "train_df = pd.read_csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_train.csv\")\n",
    "test_df = pd.read_csv(\"/Volumes/jupiter/temp/temp/rfm_analysis_test.csv\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "print(\"Train sample:\")\n",
    "display(train_df.head(3))\n",
    "print(\"Test sample:\")\n",
    "display(test_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47928d9-0c4e-4a8f-aace-bc281a254bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get common users\n",
    "common_users = set(train_df[\"user_id\"]).intersection(set(test_df[\"user_id\"]))\n",
    "print(f\"Common users: {len(common_users)}\")\n",
    "\n",
    "# Filter both datasets\n",
    "train_df = train_df[train_df[\"user_id\"].isin(common_users)].reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"user_id\"].isin(common_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered Train: {train_df.shape}, Filtered Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1973d57c-7ab8-42b3-99ce-8e5febeeab3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def top_k_recs(df, user_col, item_col, score_col, K=3):\n",
    "    \"\"\"Top-K items by RFM score for each user, plus confidence score.\"\"\"\n",
    "    def _get_top(x):\n",
    "        x_sorted = x.sort_values(score_col, ascending=False)\n",
    "        top_items = x_sorted.head(K)[item_col].tolist()\n",
    "        conf = x_sorted.head(K)[score_col].sum() / max(1, x_sorted[score_col].sum())\n",
    "        return pd.Series({\"top_items\": top_items, \"confidence\": conf})\n",
    "    return df.groupby(user_col).apply(_get_top).reset_index()\n",
    "\n",
    "def compute_hits(recs, actuals, item_label):\n",
    "    \"\"\"Compute hit score: overlap fraction (parallelized).\"\"\"\n",
    "    merged = pd.merge(recs, actuals, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "    def _hit(row):\n",
    "        return len(set(row[\"top_items\"]).intersection(row[\"actual_items\"])) / max(1, len(row[\"top_items\"]))\n",
    "\n",
    "    merged[f\"{item_label}_hit_score\"] = merged.parallel_apply(_hit, axis=1)\n",
    "    return merged[[\"user_id\", \"confidence\", f\"{item_label}_hit_score\"]]\n",
    "\n",
    "def engagement_lift(test_df, recs, user_col, item_col, spend_col=\"total_spend\"):\n",
    "    \"\"\"Spend in recommended vs non-recommended (parallelized).\"\"\"\n",
    "    test_grouped = test_df.groupby(user_col)\n",
    "\n",
    "    def _calc(row):\n",
    "        uid = row[user_col]\n",
    "        recs_set = set(row[\"top_items\"])\n",
    "        if uid not in test_grouped.groups:\n",
    "            return (uid, 0, 0)\n",
    "        user_txns = test_grouped.get_group(uid)\n",
    "        spend_rec = user_txns[user_txns[item_col].isin(recs_set)][spend_col].sum()\n",
    "        spend_nonrec = user_txns[~user_txns[item_col].isin(recs_set)][spend_col].sum()\n",
    "        return (uid, spend_rec, spend_nonrec)\n",
    "\n",
    "    results = recs.parallel_apply(_calc, axis=1)\n",
    "    return pd.DataFrame(results.tolist(), columns=[user_col, f\"{item_col}_spend_rec\", f\"{item_col}_spend_nonrec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e68398f-1d62-46bb-99cf-02b7dcb3fb7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training top merchants\n",
    "train_merchants = top_k_recs(train_df, \"user_id\", \"merchant_standardized\", \"RFM_score_merchant\", K)\n",
    "\n",
    "# Actual merchants in test\n",
    "actual_merchants = test_df.groupby(\"user_id\")[\"merchant_standardized\"].apply(set).reset_index(name=\"actual_items\")\n",
    "\n",
    "# Evaluate hits + engagement\n",
    "merchant_eval = compute_hits(train_merchants, actual_merchants, \"merchant\")\n",
    "merchant_spend = engagement_lift(test_df, train_merchants, \"user_id\", \"merchant_standardized\")\n",
    "\n",
    "print(\"Merchant evaluation sample:\")\n",
    "display(merchant_eval.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de135c39-9aa5-4c2b-98b5-20f6da03f1f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training top categories\n",
    "train_categories = top_k_recs(train_df, \"user_id\", \"appcategory\", \"RFM_score_category\", K)\n",
    "\n",
    "# Actual categories in test\n",
    "actual_categories = test_df.groupby(\"user_id\")[\"appcategory\"].apply(set).reset_index(name=\"actual_items\")\n",
    "\n",
    "# Evaluate hits + engagement\n",
    "category_eval = compute_hits(train_categories, actual_categories, \"category\")\n",
    "category_spend = engagement_lift(test_df, train_categories, \"user_id\", \"appcategory\")\n",
    "\n",
    "print(\"Category evaluation sample:\")\n",
    "display(category_eval.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08abff79-73d6-46df-9d80-ec3ddd6915be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚úÖ Overall Metrics\")\n",
    "\n",
    "print(f\"Merchant Hit Rate: {merchant_eval['merchant_hit_score'].mean():.2%}\")\n",
    "print(f\"Category Hit Rate: {category_eval['category_hit_score'].mean():.2%}\")\n",
    "\n",
    "print(f\"Avg Merchant Confidence: {merchant_eval['confidence'].mean():.2f}\")\n",
    "print(f\"Avg Category Confidence: {category_eval['confidence'].mean():.2f}\")\n",
    "\n",
    "print(\"\\nEngagement Lift (Merchant):\")\n",
    "print(merchant_spend.mean())\n",
    "\n",
    "print(\"\\nEngagement Lift (Category):\")\n",
    "print(category_spend.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0867a5ec-d392-48dc-ba8b-3511da640fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hit rate bar chart\n",
    "hit_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"Merchant\", \"Category\"],\n",
    "    \"Hit Rate\": [merchant_eval[\"merchant_hit_score\"].mean(), category_eval[\"category_hit_score\"].mean()]\n",
    "})\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.barplot(x=\"Metric\", y=\"Hit Rate\", data=hit_summary, palette=\"viridis\")\n",
    "plt.title(\"Hit Rate Comparison\")\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(hit_summary[\"Hit Rate\"]):\n",
    "    plt.text(i, v+0.02, f\"{v:.2%}\", ha=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# Confidence distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(merchant_eval[\"confidence\"], bins=20, kde=True, color=\"blue\", label=\"Merchant\")\n",
    "sns.histplot(category_eval[\"confidence\"], bins=20, kde=True, color=\"green\", label=\"Category\")\n",
    "plt.legend()\n",
    "plt.title(\"Confidence Score Distribution\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.show()\n",
    "\n",
    "# Engagement lift boxplots\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=merchant_spend.melt(id_vars=\"user_id\", value_name=\"Spend\", var_name=\"Type\"), x=\"Type\", y=\"Spend\", palette=\"Set2\")\n",
    "plt.title(\"Engagement Lift ‚Äì Merchants\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=category_spend.melt(id_vars=\"user_id\", value_name=\"Spend\", var_name=\"Type\"), x=\"Type\", y=\"Spend\", palette=\"Set1\")\n",
    "plt.title(\"Engagement Lift ‚Äì Categories\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfafeec-0450-4260-a07a-d3d9eea02aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def user_drilldown(user_id):\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üîé User: {user_id}\")\n",
    "    \n",
    "    # Train affinities\n",
    "    tm = train_merchants[train_merchants[\"user_id\"] == user_id]\n",
    "    tc = train_categories[train_categories[\"user_id\"] == user_id]\n",
    "    if not tm.empty:\n",
    "        print(\"\\nTrain Merchants:\", tm[\"top_items\"].values[0], \"Confidence:\", round(tm[\"confidence\"].values[0], 2))\n",
    "    if not tc.empty:\n",
    "        print(\"Train Categories:\", tc[\"top_items\"].values[0], \"Confidence:\", round(tc[\"confidence\"].values[0], 2))\n",
    "\n",
    "    # Test actuals\n",
    "    tmerch = test_df[test_df[\"user_id\"] == user_id][\"merchant_standardized\"].unique()\n",
    "    tcat = test_df[test_df[\"user_id\"] == user_id][\"appcategory\"].unique()\n",
    "    print(\"\\nTest Merchants:\", list(tmerch))\n",
    "    print(\"Test Categories:\", list(tcat))\n",
    "\n",
    "    # Scores\n",
    "    mh = merchant_eval[merchant_eval[\"user_id\"] == user_id]\n",
    "    ch = category_eval[category_eval[\"user_id\"] == user_id]\n",
    "    if not mh.empty:\n",
    "        print(\"\\nMerchant Hit Score:\", round(mh[\"merchant_hit_score\"].values[0], 2))\n",
    "    if not ch.empty:\n",
    "        print(\"Category Hit Score:\", round(ch[\"category_hit_score\"].values[0], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c59d55-64e5-41c8-93ae-bb2a76b4a213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_drilldown(\"0000b6d5-f969-4996-ac9c-0635f1eed680\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03ae8e1-52a3-4f91-9ee6-0577d1fe5cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge segments from train data into eval results\n",
    "seg_merchants = train_df.groupby(\"user_id\")[\"customer_segment_merchant\"].first().reset_index()\n",
    "seg_categories = train_df.groupby(\"user_id\")[\"customer_segment_category\"].first().reset_index()\n",
    "\n",
    "merchant_eval_seg = pd.merge(merchant_eval, seg_merchants, on=\"user_id\", how=\"left\")\n",
    "category_eval_seg = pd.merge(category_eval, seg_categories, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Compute segment-wise hit rates\n",
    "merchant_seg_hr = merchant_eval_seg.groupby(\"customer_segment_merchant\")[\"merchant_hit_score\"].mean().reset_index()\n",
    "category_seg_hr = category_eval_seg.groupby(\"customer_segment_category\")[\"category_hit_score\"].mean().reset_index()\n",
    "\n",
    "# ================================\n",
    "# üìä Plot Merchant Segment Hit Rates\n",
    "# ================================\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=merchant_seg_hr.sort_values(\"merchant_hit_score\", ascending=False),\n",
    "            x=\"merchant_hit_score\", y=\"customer_segment_merchant\", palette=\"Blues_r\")\n",
    "plt.title(\"Merchant Hit Rate by Customer Segment (Train ‚Üí Test)\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Average Hit Rate\")\n",
    "plt.ylabel(\"Customer Segment\")\n",
    "for i, v in enumerate(merchant_seg_hr.sort_values(\"merchant_hit_score\", ascending=False)[\"merchant_hit_score\"]):\n",
    "    plt.text(v + 0.01, i, f\"{v:.2%}\", va=\"center\", fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# üìä Plot Category Segment Hit Rates\n",
    "# ================================\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=category_seg_hr.sort_values(\"category_hit_score\", ascending=False),\n",
    "            x=\"category_hit_score\", y=\"customer_segment_category\", palette=\"Greens_r\")\n",
    "plt.title(\"Category Hit Rate by Customer Segment (Train ‚Üí Test)\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Average Hit Rate\")\n",
    "plt.ylabel(\"Customer Segment\")\n",
    "for i, v in enumerate(category_seg_hr.sort_values(\"category_hit_score\", ascending=False)[\"category_hit_score\"]):\n",
    "    plt.text(v + 0.01, i, f\"{v:.2%}\", va=\"center\", fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477ab622-fb52-42eb-9068-e255090e757a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge merchant & category evals\n",
    "user_report = (\n",
    "    merchant_eval.rename(columns={\"confidence\": \"merchant_confidence\", \"merchant_hit_score\": \"merchant_hit_score\"})\n",
    "    .merge(category_eval.rename(columns={\"confidence\": \"category_confidence\", \"category_hit_score\": \"category_hit_score\"}), \n",
    "           on=\"user_id\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# Add segments from training\n",
    "user_report = (\n",
    "    user_report\n",
    "    .merge(seg_merchants.rename(columns={\"customer_segment_merchant\": \"merchant_segment\"}), on=\"user_id\", how=\"left\")\n",
    "    .merge(seg_categories.rename(columns={\"customer_segment_category\": \"category_segment\"}), on=\"user_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Add engagement lift\n",
    "user_report = (\n",
    "    user_report\n",
    "    .merge(merchant_spend, on=\"user_id\", how=\"left\")\n",
    "    .merge(category_spend, on=\"user_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(\"User report sample:\")\n",
    "display(user_report.head())\n",
    "\n",
    "print(f\"\\nFinal report shape: {user_report.shape}\")\n",
    "\n",
    "output_path = \"/Volumes/jupiter/temp/temp/rfm_backfill_user_report.csv\"\n",
    "user_report.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Combined report saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a51bdc-4251-48af-97b4-76257bec033e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Backfill) MRv1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
